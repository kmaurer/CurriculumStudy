\documentclass[smallextended, natbib]{tise_style} 

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{graphicx,float,wrapfig,subfig,tabularx,ulem}
\graphicspath{{figure/}}
\usepackage{csquotes}
\usepackage{color}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{url}
\usepackage{bbm}
\usepackage{amsmath}

\newcommand{\hh}[1]{{\color{blue} #1}} 
\newcommand{\km}[1]{{\color{black} #1}} 

\newcommand{\distas}[1]{\mathbin{\overset{#1}{\sim}}}%

\newcommand{\Cov}{\text{Cov}}%

\begin{document}

%opening

%\title{Comparison of Learning Outcomes for Simulation-Based and Traditional Inference Curricula in a Designed Educational Experiment}
\author{Karsten Maurer \\ ISU \\
        Dennis Lock \\ ISU }
%\institute{K. Maurer \and D. Lock
%        \at Iowa State University, Ames, IA, USA}

%\maketitle

\begin{abstract}
\km{Conducting inference is a cornerstone upon which the practice of statistics is based. As such, a large portion of most introductory statistics courses is focused on teaching the fundamentals of statistical inference. The goal of this study is to make a formal comparison of learning outcomes under the traditional and simulation-based inference curricula. A randomized experiment was conducted to administer the two curricula to students in an introductory statistics course. Students of the simulation-based curriculum were found to have improved learning outcomes on topics in statistical inference; however, a clear violation of between student independence due to group administration of curriculum treatments casts considerable doubt on the statistical significance of these results. A simulation study is used to demonstrate the volatility of Type I error rates in educational studies where classroom level covariance structures exist but comparisons are made on the student level.}

% While the results are not comprehensive in assessing the effect on all facets of learning, they indicate that learning outcomes for core concepts of statistical inference can be significantly improved with the simulation-based approach. 

\end{abstract}

<<setup,echo=F,include=F,eval=T>>=
### Preliminaries 
## Load Packages
library(car)
library(ggplot2)
library(subselect)
library(qtlmt)
library(multcomp)
library(lattice)
library(gridExtra)
library(reshape2)
library(plyr)
library(boot)
library(grid)


## Load Data
# setwd("C:\\Users\\Karsten\\Dropbox\\Dissertation\\CurriculumStudy\\DraftTISE")
dat <- read.csv("data/DataForStudyAnalysis.csv",header=T)
#change factor level order to put sweeney as default control group
dat$room <- factor(dat$room.x, levels=c("sweeney", "kildee"))
dat$lab5perc <- dat$lab5perc * 100
dat$hw2perc <- dat$hw2perc * 100
dat$treatment <- "Simulation-Based"
dat$treatment[which(dat$room=="sweeney")] <- "Traditional"

errorsummary2 <- read.csv("data/errorsummary2.csv")
@

%---------------------------------------------------------------------------
\section{Introduction}
\label{intro}

Conducting inference is a cornerstone upon which the practice of statistics is based. As such, a large portion of most introductory statistics courses is focused on teaching the fundamentals of statistical inference. In recent years the approach by which to teach inference in introductory statistics courses has been the topic of growing discussion.  The traditional approach to inference curriculum is focused on distributional theory-based methodology, often characterized by use of distributional assumptions, formulas and tables. A modern alternative is a simulation-based approach to the inference curriculum. The simulation-based approach utilizes tactile and computational simulation to run inferential techniques such as bootstrapping for confidence intervals and simulation-based hypothesis testing. Many proponents of the simulation-based inference curriculum argue that this allows students to be exposed to the core concepts of the inference without first requiring the understanding of theoretical probability distributions. 

The focus of the following study is to make a formal comparison of learning outcomes under the traditional and simulation-based inference curricula.  The learning outcomes for concepts surrounding inference with confidence intervals and hypothesis testing are of primary interest.  A randomized experiment was conducted to administer the two curricula to students in an introductory statistics course. The experimental design allows for causal inference to be drawn about the effect of curriculum type on the learning outcomes. The results indicate that significant improvement in learning outcomes for confidence interval related topics are achieved using the simulation-based teaching methods.
%---------------------------------------------------------------------------
\section{Literature Review}
\label{litreviewCurricula}

With the goal to make proper comparison of traditional versus simulation-based curricula for introductory statistical courses, we must first view where each approach stands within the constant evolution of statistics education. Using the term ``traditional'' to describe the current standard for introductory statistics course curriculum is relative to only the last two decades. Moore chronicled the reform movement of statistic education of the 1980's and 1990's as a period of drastic change in the introductory statistics classroom.  The curriculum expanded greatly from a course dominated by theory-based inference methodology to the inclusion of the topics of data exploration, data production, model diagnostics and simulation.  The content change indicated a shifting emphasis toward conceptual understanding and applied statistics.  Moore also stated, ``(w)hat is striking about the current reform movement is not only its momentum but the fact that it centers on pedagogy as much as content'' \citep{Moore1997}. The pedagogical push toward active learning was combined with content change and the increasing use of technology to form what may be referred to now as the traditional introductory statistics curriculum. 

The tenets of the statistics education reform movement were formalized in the Guidelines for Assessment and Instruction in Statistics Education (GAISE) reports for pre-K-12 \citep{GAISEk12} and introductory college courses \citep{GAISEcollege}. Six recommendations were made in the executive summary of the GAISE college report: emphasize statistical literacy and thinking, use real data, stress conceptual over procedural understanding, foster active learning, use technology for both learning and analysis, and use assessment as part of the learning process.  In the past decade these principles have been widely adopted in statistics education with a noteworthy increase in technological integration.  Technology in the statistics classroom now regularly takes the form of applets, graphing calculators, multimedia materials, and educational, analytical and graphical software (\citealt{chance2007}; \citealt{Rubin2007}).  Technological proliferation in the statistics classroom came as a result of technologically receptive statistics educators taking advantage of computation that has become cheaper and more accessible.  A large survey of introductory statistics instructors found that 76\% of the instructors usually or always require students to use a computer program to explore and analyze data, and 90\% of the instructors report a high level of comfort using computer applications to teach introductory statistics \citep{Hassad2013}.  

Amidst the drastic increase in the use of technology in introductory statistics education there has been a growing group of educators who believe that the curriculum reform has stopped short of the possibilities that computation can provide.  Cobb argues that statistics education has done well to adopt technology to displace tedious calculation but has not effectively changed the approach to teaching inference.  Cobb strongly articulates a call for statistics instructors to use simulation-based methods for teaching inference to replace the traditional approach to inference using theory-based methodology.  He states, ``(o)ur curriculum is needlessly complicated because we put the normal distribution... at the center of our curriculum, instead of the core logic of inference at the center'' \citep{Cobb2007}.  If we view the introductory statistics course as a constrained optimization problem with statistical literacy and conceptual understanding of inference as the items to maximize, then removing the burden of learning the normal distribution will present the opportunity for more time spent learning core concepts \citep{Carver2011}.   In recent years, curricula for using a simulation-based approach to inference have been developed by a number of groups of statistics educators (\citealt{ISI}; \citealt{Lock5}; \citealt{CATALST};  \citealt{Carver2011}).  

There has been research done on the efficacy of simulation-based inference curricula; however, due to the recency of the curricula development most of this preliminary research has been observational.  Budgett, Pfannkuch, Regan \& Wild conduct a case study on a small group of students receiving a simulation-based curriculum and found significant learning gains using pre and post testing based on the Comprehensive Assessment of Outcomes in a First Statistics Course (CAOS).  This study does not however attempt to make a comparison between the simulation-based approach and traditional approach to teaching inference \citep{Budgett2013}. Another pair of studies make comparisons on both learning outcomes and learning retention between the two types of curricula. Tintle, VanderStoep, Holmes, Quisenberry and Swanson found weak evidence for an overall improvement in learning outcomes and significant improvements within the topic of hypothesis testing for the cohort of students receiving the simulation-based curriculum, but the lack of random assignment of student to cohort obstructs the ability to draw any causal conclusions \citep{Tintle2011}.  Tintle, Topliff, VanderStoep, Holmes and Swanson then found significant evidence for improvements to learning outcome retention after four months for students receiving the simulation-based inference curriculum, but again self-selection of students to cohort prevents establishing a causal link \citep{Tintle2012}.  

The preliminary research shows promising results for the simulation-based approach to teaching statistical inference.  A more rigorous experimental approach to comparing the traditional and simulation-based curricula has been taken in this study in order to establish a causal effect of curriculum on learning outcomes. Section 3 explains the structure and methodology implemented in the educational experiment and the measurement of student learning.  Section 4 details the model based approach for assessing the effect of curriculum on specific learning outcomes.  Lastly, we discuss the study findings and explore the implications for designing future introductory statistics curricula.
%---------------------------------------------------------------------------
\section{Methodology}
\label{methods}

The subjects for this study were students enrolled in two sections of the Introduction to Statistics, Stat 104, course at Iowa State University in the spring semester of 2014. Stat 104 is an introductory statistics course tailored for students in the agricultural and biological sciences. Of the 112 students to complete the course, 101 students consented to the release of their course data for the purposes of this study.   The students who did not consent were treated identically to those who consented, but their data was omitted from the analysis that follows.  Students from both sections were randomly assigned to one of the two inference curriculum treatments, creating cohorts A,C and B,D, respectively. Cohorts A and B were exposed to the simulation-based curriculum; while the cohorts C and D were exposed to the traditional curriculum. Student cohorts were the basic units to which room assignments, instruction and curriculum treatments were applied.

The course was administered by the authors in a co-teaching setting for students from all cohorts. The course schedule involved two hours of lecture and two hours of lab per week.  The co-teaching strategy was employed as an intentional attribute of the experimental design. The following subsections will detail the curriculum outline for each cohort of students, the experimental design for administering the curricula using the strengths of the co-teaching setup and the data collected for analysis.
%-----------------------------------------------------
\subsection{Curricula Structures}
\label{curric}

To compare the learning outcomes for students receiving the traditional and simulation-based inference curricula we first needed to prepare a curriculum for each approach.  Both curricula needed to satisfy the course guidelines set by the Department of Statistics at Iowa State University, covering the following topics: univariate and bivariate descriptive statistics, linear regression, experimental design, basic probability rules, the binomial distribution, the normal distribution, sampling distributions, and inference on means and proportions. Each curriculum was composed of lecture, corresponding lecture notes, weekly lab assignments designed for groups of four to five students, weekly homework assignments, a midterm exam and a cumulative final exam.  The curricula materials for the course did not require the use of a textbook, however specific textbooks that roughly follow the structure of each curriculum were recommended as supplementary study materials (\citealt{AgrestiFranklin}; \citealt{Lock5}).

Figure~\ref{fig:CurricSched} outlines how these topics were structured within a weekly schedule for the sixteen week semester for each curriculum.  Note that students from all cohorts were exposed to an identical curriculum for all non-inference related topics in the course. This includes identical lecture, course notes, homework assignments, lab assignments and midterm exam during the first half of the semester.

Starting at week 9 the curricula diverge into their respective approaches to inference.  Cohorts A and B began the simulation-based inference curriculum in week 9 by first learning the concepts of sampling distributions then used computer simulation and sampling variability as a basis for exploring inference using bootstrap confidence intervals and simulation-based tests.  To be specific, confidence intervals were constructed by estimating the standard error using the standard deviation of the bootstrap distribution, not through percentile-based bootstrap methods.  Lectures, homework and labs for these cohorts utilized the StatKey software package \citep{Lock5} to conduct the simulation-based inference. The simulation-based curriculum then covered normal distributions and how they could be used to conduct inference on means and proportion.  While many advocates for simulation-based methods may argue that the normal distribution should be pushed to a second course in statistics, course guidelines required that all students of this introductory statistics course be taught theory-based inference methodology.  

\begin{figure}[hbtp]
\centering
\includegraphics[keepaspectratio=true, width=1\textwidth]{CurriculumStudy/CurriculaPaths2.pdf}
\caption{\label{fig:CurricSched} Curricula schedules.}
\end{figure}

Cohorts C and D progressed through the traditional approach by first learning the normal distribution and use of the normal tables. They were then introduced to applications of the normal approximation within inference.  The traditional curriculum utilized simulation to display concepts, but only to the extent of demonstrating that sampling distributions can be approximated by normal distributions under certain conditions.  

During the second half of the semester the lectures, course notes, homework and lab assignments differed between the two curricula.  However, homework and lab assignments were kept similar when they covered similar topics.  For example, all cohorts covered the topic of sampling distributions so the lab assignments were nearly identical between the two groups with the exception of a question pertaining to the normal approximation included for the traditional cohorts.  By the end of the semester all cohorts covered how to conduct inference using normal theory; however cohorts A and B additionally learned the core concepts of inference using simulation-based methods prior to learning traditional theory-based inference methods.  

%-----------------------------------------------------
\subsection{Experimental Design}
\label{design}

The logistics of administering a course with two distinct curricula and four cohorts of students required a well-structured design and creative scheduling on several fronts.  The primary objectives for the experimental design were to eliminate differences in non-inference related curriculum administration to the extent possible, remove the confounding instructor effect on each curriculum and to mitigate the effect of unknown lurking variables through random assignment of students to curricula.  

Students were randomly assigned to cohorts during the first week of the course.  Of the 101 students who completed the course and consented to the release of their data there were 50 students in the traditional treatment group and 51 students in the simulation-based treatment group. It is also worthwhile to note that of the 4 students to drop the course, all did so prior to week 9; thus, we can safely assume that the inference curriculum treatment did not play a role in the drop. All students who began the inference curricula completed the course.

\begin{figure}[hbtp]
\centering
\includegraphics[keepaspectratio=true, width=1\textwidth]{CurriculumStudy/LectureInstructionWithRooms.pdf}\\
\vspace{.5cm}
\includegraphics[keepaspectratio=true, width=1\textwidth]{CurriculumStudy/LabInstructionWithRooms.pdf}
\caption{\label{fig:InstSched} Instructor and room schedules.}
\end{figure}

Students were exposed to identical lecture and lab instruction for the first half of the semester and then diverge into two separate lecture and lab settings for the second half of the semester. This was done to make the experience as similar as possible such that both treatment groups would have the same exposure to terminology and ideas leading up to the inference topics. We could not reassign students to lecture and lab times different than the times for which they enrolled, which meant the logistics of the design required preemptive room scheduling and course time scheduling preparations.  By working with the department chair and course coordinator before students enrolled into sections, we were able to schedule two sections of the course to have identical lecture times but separate lab times. Special room scheduling was required because all students needed to attend the same lecture and lab rooms for the first half of the semester then split into separate lecture and lab rooms after the midterm.  This room and course time scheduling allowed for students to be divided into cohorts and attend the lecture or lab specific to their curriculum.  The lecture and lab room schedules for each cohort are displayed in Figure~\ref{fig:InstSched}.

Assigning one instructor to each curriculum would confound the instructor effect and the curriculum effect.  To avoid confounding, each treatment group would need to receive instruction from both instructors.  An alternating weekly schedule was decided upon to spread out the instructor effects over both curricula. A coin was flipped to decide how to match the instructor to the curriculum when the alternation was initialized. The lecture and lab instruction schedules for each cohort can also be found below in Figure~\ref{fig:InstSched}. Note that each figure has student cohort and times fixed across all weeks, reflecting the unchanged time structure that each student enrolled into. The instructors and room locations are what changed throughout the course. 

%-----------------------------------------------------
\subsection{Data Collection}
\label{datacollect}

In order to measure learning outcomes for specific inference concepts we utilized question sets from the Assessment Resource Tools for Improving Statistical Thinking (ARTIST) for the topics of confidence intervals and hypothesis testing \citep{ARTIST}.  The ARTIST scaled question sets each consist of 10 multiple choice questions that are geared toward critical thinking and statistical literacy within each inference topic.  These questions were administered as part of the written final exam for all students on the same day and time. The ARTIST scaled scores for the topics of confidence interval and hypothesis testing were recorded for each student. The multiple choice questions for the ARTIST scaled topics can be found in Appendix~\ref{appendA}.

The final exam also included two problems that tested the student's ability to conduct statistical inference in an applied setting using theory-based methodology. Each problem was based on a hypothetical scenario where data has been collected and inference needed to be conducted using the traditional approach using formulas and tables. The first problem provided data summaries and students needed to construct and interpret a confidence interval for a single population mean. The second problem required students to conduct a hypothesis test for a single proportion based on another set of data summaries. The applied inference problem scores for each student are not used for the primary analysis on learning outcomes but are included for an interesting peripheral analysis on student ability to conduct inference using traditional theory-based methods. The applied inference problems and grading rubrics can also be found in Appendix~\ref{appendA}. The exams were graded blindly, with no identifying information of the student or treatment visible during the grading process.

In addition to the ARTIST and applied inference question scores, data were collected from the first eight weeks of the course -- prior to student exposure to an inference curriculum. We have scores from homework assignments 1 to 7, lab assignments 1 to 7 and the midterm exam for each student. The midterm exam questions and grading rubrics can be found in Appendix~\ref{appMidtermExamQuestions}. Since all of these items were administered and graded equivalently for all students before being assigned to a curricula, the scores from these weeks will be referred to as the ``pre-treatment measurements''. Lastly, the data include the cohort to which each student belonged.

The research proposal approved by the Institutional Review Board specified that students' data would be entirely deidentified following the course, including all demographic information.  At the conclusion of the semester the data for the 101 students who consented to the release of their data were saved, with names and identity information removed, to a spreadsheet.  The deidentified student data was imported to \texttt{R} for the analysis described in Section~\ref{analysis} below. 


%-----------------------------------------------------
\subsection{Data Summary}
\label{dataSummary}

The ARTIST and applied inference question scores from the final exam are the response variables on which we wish to compare the groups of students from the two inference curricula. Figure~\ref{fig:ScoreHistsByGroup} displays the histogram, mean and standard deviation for each response variable, separated by curricula.  Midterm exam scores as also included in order to provide a comparison of the curricula groups using a pre-treatment measurement. 

<<ScoreHistsByGroup, echo=FALSE , warning=FALSE, fig.width=15, fig.height=8, out.width='1\\linewidth', fig.pos='H',fig.align='center',fig.cap="Histograms and summary statistics of scores by curricula group.">>=

dat$treatment <- "Simulation-Based"
dat$treatment[which(dat$room=="sweeney")] <- "Traditional"
library(plyr)

smalldat <- dat[,c("treatment","midterm","ConfMC","HypMC","AppliedCI","AppliedHT")]
meltsmall <- melt(smalldat, id=c("treatment"))
# Wont ply properly in knit, write then read to work around... dumb
# trtsumstats <- ddply(dat, .(treatment), summarize,
#                    avgmidterm = round(mean(midterm),1 ), 
#                    sdmidterm = round(sd(midterm), 2 ), 
#                    avgConfMC = round(mean(ConfMC),1 ), 
#                    sdConfMC = round(sd(ConfMC), 2 ), 
#                    avgHypMC = sprintf("%.1f",round(mean(HypMC),1 )), 
#                    sdHypMC = round(sd(HypMC), 2 ), 
#                    avgAppliedCI = round(mean(AppliedCI),1 ), 
#                    sdAppliedCI = round(sd(AppliedCI), 2 ), 
#                    avgAppliedHT = round(mean(AppliedHT), 1 ), 
#                    sdAppliedHT = round(sd(AppliedHT), 2 )  ) 

# write.csv(trtsumstats, "trtsumstats.csv", row.names=FALSE)
trtsumstats <- read.csv("trtsumstats.csv", header=T)

titlesize =1.5
xaxissize =1.3
sumstatfont = 6
yscale <- c(0,25)


themeForMultHist <- theme(plot.margin=unit(c(1,0,1,0), "cm"),
                          axis.title.y= element_blank() ,
                          strip.text.y = element_text(size = rel(xaxissize)),
                          #strip.background = element_blank(),
                          plot.title = element_text(size = rel(titlesize)),
                          axis.title.x = element_text(size = rel(xaxissize)))


p1 <- qplot(midterm, data=dat, facets = treatment ~ ., binwidth=8, origin=4) + theme_bw()  +  
  xlab("Score (Out of 100)") + ggtitle("Midterm Exam") +
  #ylim(yscale) +
  #add mean labels
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=12, lab=c(paste("mean =",trtsumstats[1,2]),paste("mean =",trtsumstats[2,2])) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1, size= rel(sumstatfont)  )  + 
  #add sd labels
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=.85*12, lab=c(paste("sd =",trtsumstats[1,3]), paste("sd =",trtsumstats[2,3])) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1, size= rel(sumstatfont)    )+
  #add n label
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=.7*12, lab=paste("n =",rep(c("51","50"))) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1, size= rel(sumstatfont)    ) +
  themeForMultHist + 
  scale_x_continuous(limits=c(-1, 100),breaks=seq(from=0, to=100, by=25))

p2 <- qplot(ConfMC, data=dat, facets = treatment ~ ., binwidth=1) + theme_bw() + 
  xlab("Score (Out of 10)") + ylab("")  + ggtitle("ARTIST CI") +
  #ylim(yscale) +
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=12, lab=c(paste("mean =",trtsumstats[1,4]),
                                    paste("mean =",trtsumstats[2,4])) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1 , size= rel(sumstatfont)   )  + 
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=.85*12, lab=c(paste("sd =",trtsumstats[1,5]),
                                    paste("sd =",trtsumstats[2,5])) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1, size= rel(sumstatfont)    )+
  #add n label
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=.7*12, lab=paste("n =",rep(c("51","50"))) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1, size= rel(sumstatfont)    ) +
  themeForMultHist + 
  scale_x_continuous(limits=c(-1, 11),breaks=seq(from=0, to=10, by=2))


p3 <- qplot(HypMC, data=dat, facets = treatment ~ ., binwidth=1) + theme_bw() + 
  #ylim(yscale) + 
  xlab("Score (Out of 10)") + ylab("")  +  ggtitle("ARTIST HT") +
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=13, lab=c(paste("mean =",trtsumstats[1,6]),
                                    paste("mean =",trtsumstats[2,6])) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1 , size= rel(sumstatfont)   )  + 
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=.85*13, lab=c(paste("sd =",trtsumstats[1,7]),
                                    paste("sd =",trtsumstats[2,7])) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1 , size= rel(sumstatfont)   )+
  #add n label
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=.7*13, lab=paste("n =",rep(c("51","50"))) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1, size= rel(sumstatfont)    ) +
  themeForMultHist + 
  scale_x_continuous(limits=c(-1, 11),breaks=seq(from=0, to=10, by=2))

p4 <- qplot(AppliedCI, data=dat, facets = treatment ~ ., binwidth=1) + theme_bw() + 
  #ylim(yscale) + 
  xlab("Score (Out of 12)") + ylab("")  +  ggtitle("Applied CI") +
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=24, lab=c(paste("mean =",trtsumstats[1,8]),
                                    paste("mean =",trtsumstats[2,8])) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1 , size= rel(sumstatfont)   )  + 
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=.85*24, lab=c(paste("sd =",trtsumstats[1,9]),
                                    paste("sd =",trtsumstats[2,9])) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1 , size= rel(sumstatfont)   )+
  #add n label
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=.7*24, lab=paste("n =",rep(c("51","50"))) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1, size= rel(sumstatfont)    ) +
  themeForMultHist+ 
  scale_x_continuous(limits=c(-1, 13),breaks=seq(from=0, to=12, by=4))

p5 <- qplot(AppliedHT, data=dat, facets = treatment ~ ., binwidth=1) + theme_bw() + 
  #ylim(yscale) + 
  xlab("Score (Out of 11)") + ylab("")  +  ggtitle("Applied HT") +
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=19, lab=c(paste("mean =",trtsumstats[1,10]),
                                    paste("mean =",trtsumstats[2,10])) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1 , size= rel(sumstatfont)   )  + 
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=.85*19, lab=c(paste("sd =",trtsumstats[1,11]),
                                    paste("sd =",trtsumstats[2,11])) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1 , size= rel(sumstatfont)   )+
  #add n label
  geom_text(aes(x, y, label=lab), 
    data=data.frame(x=-1,y=.7*19, lab=paste("n =",rep(c("51","50"))) ,
    treatment=c("Simulation-Based","Traditional")), hjust=0, vjust=1, size= rel(sumstatfont)    ) +
  themeForMultHist + 
  scale_x_continuous(limits=c(-1, 12),breaks=seq(from=0, to=12, by=4))

grid.arrange(textGrob("Count", rot = 90, gp = gpar(cex = 1.5)),
             arrangeGrob(p1,p2,p3,p4,p5, nrow=1), 
             #arrangeGrob(textGrob("Simulation-Based", vjust = 1,
             #                     gp = gpar(cex = 1.5), rot = 90),
             #            textGrob("Traditional", vjust = 1, 
             #                     gp = gpar(cex = 1.5), rot = 90)),
             widths=unit.c(unit(2, "lines"), unit(1, "npc") - unit(2, "lines")),
             nrow=1
             )
@
\vspace{.05in}
In Figure~\ref{fig:ScoreHistsByGroup}, we see that the midterm exam scores are very similarly distributed for each group; with the traditional curriculum group scoring only slightly higher on average than the simulation-based curriculum group. This similarity is expected -- and desirable -- because the midterm was conducted prior to the treatment being administered, and the class materials and instruction were designed to be identical at that stage of the course.

Comparing the distributions in Figure~\ref{fig:ScoreHistsByGroup} we see that the simulation-based inference group had a higher average score than the traditional inference group on both of the ARTIST scaled question sets and on the applied confidence interval problem, but scored lower on average on the applied hypothesis testing problem.  The simulation-based inference group had lower variability than the traditional inference group on the ARTIST question set for confidence intervals, but higher variability on all other scores. These data summaries are suggestive of differences in the inference learning outcomes of the two groups. In Section~\ref{analysis}, we take a model-based approach to assess if these differences are statistically significant.



<<DataDescription,echo=F,include=F,eval=T>>=
options(digits=6)

# Break Down by Quartiles
dat$QuartileatMidterm <- cut(dat$midterm, breaks=quantile(dat$midterm, c(0,.25,.5,.75,1)),
    labels=c("1st Quartile","2nd Quartile","3rd Quartile","4th Quartile"), include.lowest=TRUE)
quartdat <- ddply(dat, .(QuartileatMidterm,room), summarize,
      nstudent = length(HypMC),
      avgMidterm =  round(mean(midterm),2),
      avgConfMC = round(mean(ConfMC)*10,2),
      avgHypMC =  round(mean(HypMC)*10,2),
      avgAppliedCI =  round(mean(AppliedCI)*100/12,2),
      avgAppliedHT =  round(mean(AppliedHT)*100/11,2)
      )
quartdiffs <-quartdat[c(1,3,5,7),] - quartdat[c(2,4,6,8),] 
quartdiffs$QuartileatMidterm <- levels(dat$QuartileatMidterm)
quartwithdiffs <- rbind(quartdat, quartdiffs)

#xtable(quartwithdiffs[order(quartwithdiffs$QuartileatMidterm),c(1,2,4:8)])

roomdat <- ddply(dat, .(room), summarize,
                  nstudent = length(HypMC),
                  avgMidterm =  round(mean(midterm),2),
                 avgConfMC =  round(mean(ConfMC),2),
                 avgHypMC =  round(mean(HypMC),2),
                 avgAppliedCI =  round(mean(AppliedCI),2),
                 avgAppliedHT =  round(mean(AppliedHT),2)
)
#xtable(roomdat[,c(1,3:7)])

@
% 
% % latex table generated in R 3.0.2 by xtable 1.7-3 package
% % Mon Nov 24 15:50:03 2014
% 
% \begin{table}[ht]
% \small
% \centering
% \begin{tabular}{llrrrrr}
%   \hline
%     & Curriculum & Midterm & ARTIST CI & ARTIST HT & Applied CI & Applied HT \\ 
%   \hline
%   & Traditional & \Sexpr{roomdat[1,3]} & \Sexpr{roomdat[1,4]} & \Sexpr{roomdat[1,5]} & \Sexpr{roomdat[1,6]} & \Sexpr{roomdat[1,7]} \\ 
%    & Simulation & \Sexpr{roomdat[2,3]} & \Sexpr{roomdat[2,4]} & \Sexpr{roomdat[2,5]} & \Sexpr{roomdat[2,6]} & \Sexpr{roomdat[2,7]} \\ 
%  \hline
%  & & & & & &  \\ 
%  & & & & & &  \\ 
%  \hline
%   
%  Midterm Quartile & Curriculum & Midterm & ARTIST CI & ARTIST HT & Applied CI & Applied HT \\ 
%   \hline
% 1st Quartile & Traditional & 55.50 & 55.38 & 56.92 & 81.41 & 61.54 \\ 
%  & Simulation & 55.68 & 64.29 & 50.00 & 74.40 & 62.34 \\ 
%  & Difference & -0.18 & -8.90 & 6.92 & 7.01 & -0.80 \\ 
%  \hline
% 2nd Quartile & Traditional & 73.19 & 57.69 & 51.54 & 77.56 & 69.23 \\ 
%  & Simulation & 71.00 & 71.67 & 52.50 & 86.11 & 66.67 \\ 
%  & Difference & 2.19 & -13.97 & -0.96 & -8.55 & 2.56 \\ 
%  \hline
% 3rd Quartile & Traditional & 84.58 & 76.15 & 57.69 & 88.46 & 90.91 \\ 
%  & Simulation & 82.71 & 71.67 & 61.67 & 84.72 & 75.00 \\ 
%  & Difference & 1.87 & 4.49 & -3.97 & 3.74 & 15.91 \\ 
%  \hline
% 4th Quartile & Traditional & 94.33 & 78.33 & 65.83 & 90.97 & 93.94 \\ 
%  & Simulation & 91.67 & 82.50 & 76.67 & 95.83 & 93.18 \\ 
%  & Difference & 2.67 & -4.17 & -10.83 & -4.86 & 0.76 \\ 
%    \hline
% \end{tabular}
% \caption{Average percentage scores broken down by midterm exam quartiles.}
% \label{tab:examquartiles}
% \normalsize
% \end{table}



%---------------------------------------------------------------------------
\section{Analysis}
\label{analysis}

The primary goal of the analysis is to investigate if there is a curricula effect on inference concept learning outcomes.  Our data includes ARTIST scaled topic scores for confidence intervals and hypothesis tests which we use as the responses for the comparison of curricula. A model based approach is used to assess curricula effect while controlling for pre-treatment differences between students. With the two dimensional response and an assortment of covariates we employ a multivariate analysis of covariance (MANCOVA) model.   

Both curricula groups were required to learn how to conduct normal-based inference. This leads to another question of interest. Does the added simulation-based material turn out to be detrimental to student's ability to use distributional theory-based methods to conduct inference?  Two applied problems were included on the final exam that required students to use theory-based methods and formulas to conduct inference. These applied questions were used as the responses in a separate MANCOVA model to check for a curriculum effect. The bivariate MANCOVA models used for these two analyses are parameterized as 
%
\begin{eqnarray}\label{eq:mancova}
y_{i\ell} = \tau_{\ell} \mathbbm{1}_{\{i \in T\}} + \beta_{\ell 0} + \sum_{p=1}^{P}x_{ip}\beta_{\ell p} + \epsilon_{i\ell},
\end{eqnarray}
where 

\begin{tabular}{lp{5in}}
$y_{i\ell}$ &  is the $\ell^{th}$ response ($\ell \in \left\{1,2\right\}$) from  student $i$, $ 1 \le i \le n$,\\
$\tau_{\ell}$ & is the treatment effect of the simulation-based curriculum on response $\ell$, and \\
$\mathbbm{1}_{\{i \in T\}}$ & is the indicator function for student $i$ in the treatment group.\\
$\beta_{\ell 0}$ & is the common intercept for response $\ell$, and \\
$\beta_{\ell p}$, & $1 \le p \le P$ are the model coefficients of the $P$ covariates. \\
$x_{ip}$ & is the $p^{th}$ pre-treatment covariate score of student $i$, and\\
$\epsilon_{i\ell}$ & is the error for the $\ell ^{th}$ response from the $i^{th}$ student. 
\end{tabular}\\

We assume that error pairs are independent and identically distributed:

\[
\vec{\epsilon}_{i} = 
\begin{bmatrix}
  \epsilon_{i1} \\ \epsilon_{i2} 
 \end{bmatrix}  
 \distas{iid} \text{MVN} \left( 
 \begin{bmatrix}
  0 \\ 0 
 \end{bmatrix},
 \Sigma = \begin{bmatrix}
  \sigma_{11}^2 & \sigma_{12}^2 \\ 
  \sigma_{21}^2 & \sigma_{22}^2
 \end{bmatrix}
 \right)
\]

% \begin{center}
% $y_{ik} = \tau_{k} \mathbbm{1}_{\{i \in T\}} + \beta_{0k} + \Bigg(\sum_{p=1}^{P}x_{ip}\beta_{pk}\Bigg) + \epsilon_{ik}$,\\
% 
% where $k \in \left\{1,2\right\}$,  $i \in \left\{1,...,n\right\}$, and $p \in \left\{1,...,P\right\}$,
% 
% $\vec{\epsilon}_{i} = 
% \begin{bmatrix}
%   \epsilon_{i1} \\ \epsilon_{i2} 
%  \end{bmatrix}  
%  \distas{iid} $ MVN$\left( 
%  \begin{bmatrix}
%   0 \\ 0 
%  \end{bmatrix},
%   \begin{bmatrix}
%   \sigma_{11}^2 & \sigma_{12}^2 \\ 
%   \sigma_{21}^2 & \sigma_{22}^2
%  \end{bmatrix}
%  \right)$,
% \end{center}

With this parameterization, it is clear that the underlying structure of the MANCOVA model is a multivariate multiple linear regression that can include categorical and continuous covariates. Note that the paired error terms from each student are correlated but are specified as independent between students. The assumption of independence between student response scores is understood to be unrealistic for students from the same class. \km{The repercussions of violating the assumption of independence between student responses will be explored through a simulation study in Section~\ref{SimStudy} following the analysis.}

%-----------------------------------------------------
\subsection{Modeling ARTIST Outcomes}
\label{ArtistModel}

We begin with the model for the ARTIST scaled topic scores.  Many of the pre-treatment variables are highly correlated. To select a model with only the most predictive pre-treatment covariates, model selection was conducted by first running backward selection based on AIC then removing further covariates that posed collinearity issues. The model selected for final analysis included three covariates: an indicator variable for the curriculum treatment group, the lab 5 score and the midterm score.  The midterm tested students on materials from weeks 1-7 and lab 5 assessed understanding of topics related to random selection techniques.  We will refer to this selected model as the ``ARTIST Model''. Model fit for the ARTIST Model was assessed to be satisfactory; see Appendix~\ref{appARTISTModDiag} for residual plots and other model diagnostics.

<<ARTISTModSelectDiag,echo=F,include=F,eval=T>>=
### Build MANCOVA model for ARTIST scaled multiple choice scores for CI and HT
## Backward Stepwise Selection Using AIC as selection criterion
#start with biggest possible model
mod2 <- lm(cbind(ConfMC,HypMC)~ midterm + Section +
             hw1perc +  hw2perc + hw3perc + hw4perc + hw5perc + hw6perc + hw7perc +
            lab1perc +lab2perc +lab3perc +lab4perc +lab5perc +lab6perc + lab7perc +  room , data=dat)
summary(manova(mod2))
#backward stepwise to reduce model
mod2backward <- mStep(mod2, k=2, trace=TRUE) #k=2 means to use AIC
summary(manova(mod2backward))
#removal of lab2, hw4 due to p-values over .1 
#also remove hw5 due to worries about colinearity with lab5 (same topics)
mod2small <- update(mod2backward, .~. -  lab2perc - hw4perc - hw5perc)
summary(manova(mod2small), tests=c("Pillai","Wilks"))
summarytab <- summary(manova(mod2small))$stats
#significant overall effects: midterm and lab5 (random sampling lab)
#weak overall effect of treatment
summary(mod2small)
mod2smallCI <- lm(ConfMC~ midterm + lab5perc + room, data=dat)
summary(mod2smallCI)
CIcis <- data.frame(confint(mod2smallCI,level=.95))
CIcis$ests <- mod2smallCI$coeff
mod2smallHT <- lm(HypMC~ midterm + lab5perc + room, data=dat)
summary(mod2smallHT)
HTcis <- data.frame(confint(mod2smallHT,level=.95))
HTcis$ests <- mod2smallHT$coeff
#individually there is a significant effect of treatment on CI score
  #multiple comparisons adjustment ?!?!


#individually there is no significant effect of treatment on HT score

#--------------------------------------------------
### Check Conditions for MANCOVA linear model fit
## check response correlations to avoid colinearity in multivariate model
#Rule of Thumb: ~ .3 to .55 then MANCOVA will work well
with(dat, cor(ConfMC,HypMC)) #Not overly correlated

## Normality of responses
#qplot(dat$ConfMC)
#qplot(dat$HypMC)
#each is reasonably normal univariately
#qplot(dat$ConfMC,dat$HypMC) + geom_jitter()
# reasonable bivariate normal

## check residual plots for homoscedesticity and linearity
qplot(mod2small$fitted.values[,1] ,mod2small$residuals[,1]) + 
  geom_hline(yintercept=0) + geom_jitter()
qplot(mod2small$fitted.values[,2] ,mod2small$residuals[,2]) + 
  geom_hline(yintercept=0) + geom_jitter()
# these seem fine (put into appendix)

## Independence of responses between students must be assumed
@

<<MultiCompCheck,echo=F,include=F,eval=F>>=
## Make 95% intervals with Scheffe's Procedure to check if effects
# Scheffe intevals: (estimate) +/- SCrit SE(estimate) 
# where SCrit = sqrt((r-1)*F(.95; r-1, n-r-1)) r=number of comparisons , n=total sample size
# for us, r = 2 , n-r-1 = 98
SCrit = sqrt((2-1)*qf(.95,1,98))
ests <- as.numeric(c(mod2smallCI$coef[4], mod2smallHT$coef[4]))
SEests <- c(summary(mod2smallCI)[[4]][4,2], summary(mod2smallHT)[[4]][4,2])
ScheffeInts <- data.frame(Betas = ests,
                          Lower = ests - SCrit* SEests ,
                          Upper = ests + SCrit* SEests)
ScheffeInts
### SCHEFFE'S DOESN'T WORK IN MANCOVA SETTING
@

To test for overall covariate effects on the multivariate responses we use Pillai's $\Lambda$, a statistic which reflects the amount of variability in the bivariate responses explained by a covariate. Table~\ref{tab:overallmod} shows a weak overall effect of the curriculum treatment on the paired ARTIST scaled topic scores.  This prompts us to investigate the treatment effect on the ARTIST scaled topic scores for confidence intervals and hypothesis tests separately, to see if the weak overall effect is driven by a significant effect on one of the two scores.\\

\begin{table}[hbtp]
\centering
\caption[Pillai's tests for bivariate effects in ARTIST model.]{Tests for bivariate effects on ARTIST question scores using Pillai's $\Lambda$.}
\begin{tabular}{lrrr} \hline
 & Pillai's $\Lambda$ & Approx. F & p-value\\ 
 \hline  
Midterm & \Sexpr{sprintf("%.4f",round(summarytab[1,2],4))} & \Sexpr{sprintf("%.4f",round(summarytab[1,3],4))} &
\Sexpr{sprintf("%.4f",round(summarytab[1,6],6))}\\ 
Lab 5 & \Sexpr{sprintf("%.4f",round(summarytab[2,2],4))} & \Sexpr{sprintf("%.4f",round(summarytab[2,3],4))} &
\Sexpr{sprintf("%.4f",round(summarytab[2,6],6))}\\ 
Treatment & \Sexpr{sprintf("%.4f",round(summarytab[3,2],4))} & \Sexpr{sprintf("%.4f",round(summarytab[3,3],4))} &
\Sexpr{sprintf("%.4f",round(summarytab[3,6],6))}\\ 
\hline
\end{tabular}
\label{tab:overallmod}
\end{table}
%
To investigate the effect of the curriculum treatment on each ARTIST scaled topic score, we analyze the two underlying linear models that comprise the overall MANCOVA model. Table~\ref{tab:cimod} displays the coefficients of the linear model fit to the ARTIST scaled score for confidence interval learning outcomes along with covariate ranges to provide context to coefficient magnitudes. It should be noted that although the midterm and lab scores were recorded discretely, they were treated as continuous covariates when fitting the model. We find that midterm, lab 5 and the curriculum treatment effect are significant. Specifically, the simulation-based inference group scored significantly higher by 0.7146 out of a possible 10 points, a 7.146\% improvement in confidence interval learning outcomes on the ARTIST scale while controlling for midterm and lab 5 scores.

Table~\ref{tab:htmod} displays the coefficients of the linear model fit to the ARTIST scaled score for hypothesis test learning outcomes.  We find that only the midterm score is significant for predicting learning outcomes for hypothesis testing. There was no significant curriculum treatment effect. \\

% latex table generated in R 3.0.2 by xtable 1.7-3 package
% Tue Jul 29 15:46:21 2014
\begin{table}[hbtp]
\centering
\caption[ARTIST model coefficients for confidence interval topic scores.]{Coefficients for model fit to ARTIST score for confidence interval topic.}
\begin{tabular}{rccc}
  \hline
 & Covariate Values & Estimate & 95\% Confidence Interval \\ 
  \hline
Intercept & 1 & \Sexpr{sprintf("%.4f",round(CIcis[1,3],6))} & \Sexpr{paste("(",sprintf("%.4f",round(CIcis[1,1],6)),",", sprintf("%.4f",round(CIcis[1,2],6)),")",sep=" ")} \\ 
  Midterm & \{0,1,...100\} & \Sexpr{sprintf("%.4f",round(CIcis[2,3],6))} & \Sexpr{paste("( ",sprintf("%.4f",round(CIcis[2,1],6)),",", sprintf("%.4f",round(CIcis[2,2],6)),")",sep=" ")} \\ 
  Lab 5  &  \{0,1,...100\} & \Sexpr{sprintf("%.4f",round(CIcis[3,3],6))} & \Sexpr{paste("( ",sprintf("%.4f",round(CIcis[3,1],6)),",", sprintf("%.4f",round(CIcis[3,2],6)),")",sep=" ")} \\ 
  Treatment &  \{0,1\} & \Sexpr{sprintf("%.4f",round(CIcis[4,3],6))} & \Sexpr{paste("( ",sprintf("%.4f",round(CIcis[4,1],6)),",", sprintf("%.4f",round(CIcis[4,2],6)),")",sep=" ")} \\ 
   \hline
\end{tabular}
\label{tab:cimod}
\end{table}
%

% latex table generated in R 3.0.2 by xtable 1.7-3 package
% Tue Jul 29 15:41:44 2014
\begin{table}[hbtp]
\centering
\caption[ARTIST model coefficients for hypothesis test topic scores.]{Coefficients for model fit to ARTIST score for hypothesis test topic.}
\begin{tabular}{rccc}
  \hline
 & Covariate Values & Estimate & 95\% Confidence Interval \\ 
  \hline
Intercept & 1 & \Sexpr{sprintf("%.4f",round(HTcis[1,3],6))} & \Sexpr{paste("(",sprintf("%.4f",round(HTcis[1,1],6)),",", sprintf("%.4f",round(HTcis[1,2],6)),")",sep=" ")} \\ 
  Midterm & \{0,1,...100\} & \Sexpr{sprintf("%.4f",round(HTcis[2,3],6))} & \Sexpr{paste("( ",sprintf("%.4f",round(HTcis[2,1],6)),",", sprintf("%.4f",round(HTcis[2,2],6)),")",sep=" ")} \\ 
  Lab 5  &  \{0,1,...100\} & \Sexpr{sprintf("%.4f",round(HTcis[3,3],6))} & \Sexpr{paste("( ",sprintf("%.4f",round(HTcis[3,1],6)),",", sprintf("%.4f",round(HTcis[3,2],6)),")",sep=" ")} \\ 
  Treatment &  \{0,1\} & \Sexpr{sprintf("%.4f",round(HTcis[4,3],6))} & \Sexpr{paste("( ",sprintf("%.4f",round(HTcis[4,1],6)),",", sprintf("%.4f",round(HTcis[4,2],6)),")",sep=" ")} \\ 
   \hline
\end{tabular}
\label{tab:htmod}
\end{table}

<<IndCoeffTables,echo=F,include=F,results='asis',eval=T>>=
#library(xtable)
#xtable(summary(mod2smallHT))
#xtable(summary(mod2smallCI))
@

A final consideration in the comparison of learning outcomes using the ARTIST model is that we made two primary comparisons; the curriculum effect on each of the inference topics. While several multiple comparisons adjustments have been developed for univariate response modeling, the Bonferroni method is the only traditional adjustment that is flexible enough for use in the MANCOVA setting. With the Bonferroni adjustment, if we wish to maintain an overall $\alpha=0.05$ significance level then we hold each individual comparison to the $\alpha=0.025$ level. After using the Bonferroni adjustment, the curriculum effect on learning outcomes for confidence interval topics would no longer considered significant ($\text{p-value} = 0.031 > 0.025$) at the overall $\alpha = 0.05$ level, but instead would be significant at the overall $\alpha = 0.1$ level.  However, the Bonferroni method is well known for being overly conservative in its adjustment, and we are comfortable with maintaining the original interpretations.

%-----------------------------------------------------
\subsection{Modeling Applied Theory-Based Inference Scores}
\label{AppliedModel}

As with the MANCOVA model for ARTIST scaled question scores, we consider all pre-treatment measurements in a new model for the two applied theory-based inference question scores.  Backward stepwise selection was used to obtain a reduced MANCOVA model in a model selection process identical to that described in Subsection~\ref{ArtistModel}. We will refer to the selected model here as the ``Applied Model''.  Residual plots and other model diagnostics for the Applied Model may be found in Appendix~\ref{appAppliedModDiag}.

Table~\ref{tab:overallmod1} shows -- based the Pillai's $\Lambda$ -- that there was no overall effect of curriculum treatment on the scores for the pair of applied inference problems. This is of particular interest because students receiving the simulation-based curriculum had three weeks less of coursework involving the use of normal distributions and normal tables. This implies that despite the increased complexity of the simulation-based material and the shortened exposure to theory-based inference concepts, there was no significant detriment to students' performance in conducting inference using theory-based methods. It should be noted that the applied questions from the final exam were written by the authors and have not been assessed as reliable metrics for learning outcomes. Thus, the results are reported as supplementary to the discussion on learning outcomes measured by the ARTIST scaled topics. \\

<<AppliedModSelectDiag,echo=F,include=F,eval=T>>=
### Build MANCOVA model for Applied scores for CI and HT
## Backward Stepwise Selection Using AIC as selection criterion
#start with biggest possible model
mod1 <- lm(cbind(AppliedCI, AppliedHT)~midterm + Section + 
             hw1perc +  hw2perc + hw3perc + hw4perc + hw5perc + hw6perc + hw7perc +
             lab1perc +lab2perc +lab3perc +lab4perc +lab5perc +lab6perc + lab7perc +
             room  , data=dat)
summary(manova(mod1))
#backward stepwise to reduce model
mod1backward <- mStep(mod1, k=2, trace=TRUE) #k=2 means use AIC
summary(manova(mod1backward))
# it took out room, put it back in
mod1small <- update(mod1backward, .~. + room)
# hw7 and lab4 removed iteratively because pillai's not significant at .1 level
# (process was check pillais, remove hw7, check pillais, remove lab4, check pillais)
mod1small <- update(mod1small, .~. - hw7perc - lab4perc)
summary(mod1small)
manova(mod1small)
summary(manova(mod1small))
summarytab1 <- summary(manova(mod1small))$stats
#significant overall effects: midterm, hw2
#no overall effect of treatment
mod1smallCI <- lm(AppliedCI~ midterm + hw2perc + room, data=dat)
summary(mod1smallCI)
mod1smallHT <- lm(AppliedHT~ midterm + hw2perc + room, data=dat)
summary(mod1smallHT)

#--------------------------------------------------
### Check Conditions for MANCOVA linear model fit
## check response correlations to avoid colinearity in multivariate model
#Rule of Thumb: ~ .3 to .55 then MANCOVA will work well
with(dat, cor(AppliedCI,AppliedHT)) #Not overly correlated

## Normality of responses
#qplot(dat$AppliedCI)
#qplot(dat$AppliedHT)
#CI reasonably normal univariately, HT not normal
#qplot(dat$AppliedCI,dat$AppliedHT) + geom_jitter()
# reasonable bivariate normal?

## check residual plots for homoscedesticity and linearity
qplot(mod1small$fitted.values[,1] ,mod1small$residuals[,1]) + 
  geom_hline(yintercept=0) + geom_jitter()
qplot(mod1small$fitted.values[,2] ,mod1small$residuals[,2]) + 
  geom_hline(yintercept=0) + geom_jitter()
# these seem fine (put into appendix)

## Independence of responses between students must be assumed
@

\begin{table}[hbtp]
\centering
\caption[Pillai's tests for bivariate effects in Applied model.]{Tests for bivariate effects on Applied question scores using Pillai's $\Lambda$.}
\begin{tabular}{lrrr} \hline
 & Pillai's $\Lambda$ & Approx. F & p-value\\ 
 \hline  
Midterm & \Sexpr{sprintf("%.4f",round(summarytab1[1,2],4))} & \Sexpr{sprintf("%.4f",round(summarytab1[1,3],4))} &
\Sexpr{sprintf("%.4f",round(summarytab1[1,6],6))}\\ 
Homework 2 & \Sexpr{sprintf("%.4f",round(summarytab1[2,2],4))} & \Sexpr{sprintf("%.4f",round(summarytab1[2,3],4))} &
\Sexpr{sprintf("%.4f",round(summarytab1[2,6],6))}\\ 
Treatment & \Sexpr{sprintf("%.4f",round(summarytab1[3,2],4))} & \Sexpr{sprintf("%.4f",round(summarytab1[3,3],4))} &
\Sexpr{sprintf("%.4f",round(summarytab1[3,6],6))}\\ 
%Treatment & \Sexpr{round(summarytab1[4,2],4)} & \Sexpr{round(summarytab1[4,3],4)} &
%\Sexpr{round(summarytab1[4,6],6)}\\ 
\hline
\end{tabular}
\label{tab:overallmod1}

\end{table}



\km{
%-----------------------------------------------------
\subsection{Model Assessment}
\label{SimStudy}

The bivariate MANCOVA models that were employed in Sections~\ref{ArtistModel}~and~\ref{AppliedModel} make the assumption of independent errors between students; an assumption which is very likely violated in practice because learning outcomes for students attending the same lectures and labs are very likely related. The assumption of independence is typically made for convenience and the lack of repetition on the lecture and lab levels prevents us from estimating a proper variance structure. It is therefore important to assess the consequences of this violation on the fitted MANCOVA model; specifically, the impact on the Type 1 error rates in tests for curriculum effects on learning outcomes. We elect to explore the consequences through a simulation study wherein the assumption of independence is knowingly violated and the effects on errors rates can be recorded.}

\km{We choose the ARTIST Model (\ref{eq:mancova}) from Sections~\ref{ArtistModel} as the basis for our simulation study. Under the assumption of independence between students, we found weak evidence of a curriculum effect on the bivariate ARTIST learning outcomes using Pillai's test. Further inspection of the individual responses using t-tests revealed significant evidence of a curriculum effect on confidence interval learning outcomes, but no evidence of a curriculum effect on hypothesis test learning outcomes. In order to assess the trustworthiness of the results, we must first know how a violation of the assumption of independence impacts the Type 1 error rates for these tests. We simulate responses from a generative model without a curriculum effect on learning outcomes (i.e.\ $\tau_{1} = \tau_{2} = 0$) and where the assumption of independence between students is violated to a known degree. We then track the Type 1 error rate in curriculum effects when the ARTIST model, assuming independence, is fit to the simulated responses.}

\km{The generative model for the simulations is adapted from the MANCOVA model~(\ref{eq:mancova}) by adding random effects to violate the independence between students. The generative model includes fixed effects for lab 5 and midterm scores, and random effects for responses to each ARTIST topic, lab sections and lecture section. Recall that responses are nested within students, students are nested within lab sections, and labs are nested within lecture sections. Thus the generative model is defined as,
%
\begin{eqnarray}\label{eq:simgenerative}
y_{ijk\ell} = \left[ \beta_{0k} + x_{ijk1}\beta_{k1} + x_{ijk2}\beta_{k2} \right] + \left[ \eta_{\ell} + \gamma_{k\ell} + \delta_{jk\ell} + \epsilon_{ijk\ell} \right],
\end{eqnarray}
%
for $i \in \left\{1,\dots,n\right\}$, $j \in \left\{1,2,3,4\right\}$, $k \in \left\{1,2\right\}$, and $\ell \in \left\{1,2\right\}$.  Where $y_{ijkl}$ is the $\ell^{th}$ response from student $i$ who is in lab section $j$ and lecture section $k$. The fixed effects portion of the model, within the first square brackets, is defined identically to the original ARTIST model~(\ref{eq:mancova}). In the generative model the $\beta$ coefficients are set equal to the corresponding coefficient estimates from the original ARTIST model. The remaining terms, $\eta_{\ell}$, $\gamma_{k\ell}$, $\delta_{jk\ell}$, and $\epsilon_{ijk\ell}$ are random effects for responses, lecture sections, lab sections and individual errors, respectively, distributed as follows\\

\[
\vec{\epsilon}_{ijk} = 
\begin{bmatrix}
  \epsilon_{ijk1} \\ \epsilon_{ijk2} 
 \end{bmatrix}  
 \distas{iid} \text{MVN} \left( 
 \begin{bmatrix}
  0 \\ 0 
 \end{bmatrix},
 \Sigma = \begin{bmatrix}
  \sigma_{11}^2 & \sigma_{12}^2 \\ 
  \sigma_{21}^2 & \sigma_{22}^2
 \end{bmatrix} 
 \right),
\]
\[
\vec{\delta}_{jk} = 
\begin{bmatrix}
  \delta_{jk1} \\ \delta_{jk2} 
 \end{bmatrix}  
 \distas{iid} \text{MVN} \left( 
 \begin{bmatrix}
  0 \\ 0 
 \end{bmatrix},
 d\Sigma = \begin{bmatrix}
  d\sigma_{11}^2 & d\sigma_{12}^2 \\ 
  d\sigma_{21}^2 & d\sigma_{22}^2
 \end{bmatrix} 
 \right),
\]
\[
\vec{\gamma}_{k} = 
\begin{bmatrix}
  \gamma_{k1} \\ \gamma_{k2} 
 \end{bmatrix}  
 \distas{iid} \text{MVN} \left( 
 \begin{bmatrix}
  0 \\ 0 
 \end{bmatrix},
 g\Sigma = \begin{bmatrix}
  g\sigma_{11}^2 & g\sigma_{12}^2 \\ 
  g\sigma_{21}^2 & g\sigma_{22}^2
 \end{bmatrix} 
 \right),
\]
\[
\vec{\eta} = 
\begin{bmatrix}
  \eta_{1} \\ \eta_{2} 
 \end{bmatrix}  
 \distas{iid} \text{MVN} \left( 
 \begin{bmatrix}
  0 \\ 0 
 \end{bmatrix},
 z\Sigma = \begin{bmatrix}
  z\sigma_{11}^2 & z\sigma_{12}^2 \\ 
  z\sigma_{21}^2 & z\sigma_{22}^2
 \end{bmatrix} 
 \right).
\]

Thus the variance structure for the responses includes common variance components $\sigma_{\ell\ell'}$ and scaling parameters $z$, $g$ and $d$. The variance components for $\sigma_{\ell\ell'}$ from the original ARTIST model are plugged in as variance parameters for the generative model. The scaling parameter $z$ controls the variability that is common between all student responses; thus $z \sigma_{\ell\ell'}$ is the covariance of responses for ARTIST sets $\ell$ and $\ell'$ between students that do not share lecture or lab sections. The scaling parameter $g$ controls the additive increase to covariance between students of the same lecture section, $g \sigma_{\ell\ell'}$, and $d$ controls the additive increase to covariance between students of the same lab section, $d \sigma_{\ell\ell'}$. If each of these variance scaling parameters is set to zero, there is no violation to the assumption of independence between students; thus making the variance structure of the generative model~(\ref{eq:simgenerative}) match the error structure of the original ARTIST model~(\ref{eq:mancova}). We will use these parameters to control for violations of  the independence assumption to different degrees in the simulation.}

\km{The simulation procedure for assessing the Type 1 error rates under violations of independence between student responses is conducted through a five step process. We consider violations based on all combinations of $d \in \{0,0.02,0.04,0.06,0.08,0.1\}$ and $g \in \{0,0.02,0.04,0.06,0.08,0.1\}$. We set $z$=0 for all simulations because any random effect that is common to \textit{all} students will not effect tests for curriculum effects (i.e. the difference in average responses between curricula groups remains constant). For each combination of variance scaling parameters we repeat the following simulation process for $m \in \{1,\dots,20000\}$: 
%
\begin{enumerate}
\item Randomly permute the lecture and lab section labels in the data 
\item Simulate the random effects by drawing $\vec{\epsilon_{ijk}}^{\hspace{.1cm}(m)}$, $\vec{\delta_{jk}}^{\hspace{.1cm}(m)}$,$\vec{\gamma_{k}}^{\hspace{.1cm}(m)}$, and $\vec{\eta}^{\hspace{.1cm}(m)}$ from the multivariate normal distributions defined above.
\item Compute the $m^{th}$ simulated responses with generative model~(\ref{eq:simgenerative}) as:
%
\begin{center}
$y_{ijk\ell}^{\hspace{.1cm}(m)} = \beta_{0k} + x_{ijk1}\beta_{k1} + x_{ijk2}\beta_{k2} + \eta_{\ell}^{\hspace{.1cm}(m)} + \gamma_{k\ell}^{\hspace{.1cm}(m)} + \delta_{jk\ell}^{\hspace{.1cm}(m)} + \epsilon_{ijk\ell}^{\hspace{.1cm}(m)}$
\end{center}
%
\item Fit the ARTIST Model, assuming independence, to the simulated responses.
\item Conduct Pillai's test for overall curriculum effect, then t-tests for curriculum effect on each response individually. Record test statistics and p-values.
\end{enumerate}
}

\km{Recall that in the generative model~(\ref{eq:simgenerative}) does not include a treatment effect, therefore any test where significant curriculum effects are found has incurred a Type 1 error. Figure~\ref{fig:simerrorratesdg} displays the observed Type 1 error rates in simulations under violations of independence between students. When the variance scaling parameters are set to zero there is no violation of independence and we see that tests hold at the nominal Type 1 error rate of $\alpha$=0.05. However, the error rates increase quickly when the variance scaling parameters, $d$ and $g$, increase. This occurs more dramatically for the overall Pillai's test than for the individual t-tests.}

\km{To establish points of reference for interpreting Figure~\ref{fig:simerrorratesdg} we examine the error rates under a few specific parameter settings. When the between labmate covariance is 4\% higher than for non-labmates (i.e. $d$=0.04 and $g$=0) the Type 1 error rates for individual t-tests are above 0.15 and the Pillai's test is above 0.20; over three and four times the nominal rate, respectively. Worse, when the between classmate covariance in 4\% higher than for non-classmates (i.e. $d$=0 and $g$=0.04) the Type 1 error rates for individual t-tests are above 0.25 and the Pillai's test is above 0.35; over five and seven times the nominal rate, respectively. This error rate inflation occurs because the random effects based on lecture and lab sections are being misinterpreted in the ARTIST model as fixed effects of curriculum due to the assumption of independence between students; a misinterpretation made worse in the case of lecture section due to the complete confounding with curriculum.}

<<simerrorratesdg, echo=FALSE , message=FALSE, warning=FALSE, fig.width=10, fig.height=4, out.width='1\\linewidth', fig.pos='h',fig.align='center',fig.cap="Type 1 error rates from 20,000 simulations under each combination of $d$ and $g$ for individual t-tests and Pillai's overall test for curriculum effects on the ARTIST response scores. The horizontal black line indicates the nominal Type 1 error rate of $\\alpha$ = 0.05.",fig.scap="Type 1 error rates from simulations of independence violation.", cache=TRUE>>=
# errorsummary2 found in setup script
alpha <- 0.05
### Type 1 error plots:
# error increases as delta/gamma (class/labmate effects) increase in
# relation to the within student eta.  Tau held to zero for all
errorsummary2$d <- factor(errorsummary2$d, levels=c(0,.02,.04,.06,.08,.1))
p1 <- qplot(g,T1ErrorRatePillai, data=errorsummary2, geom="line", group=d, 
  color=d, size=I(.7)) + geom_hline(yintercept=alpha)+
  xlab("g") + theme_bw() + ylim(c(0,0.75)) + 
  scale_x_continuous(breaks=c(0,.02,.04,.06,.08,.1)) +
  ylab("")+
  ggtitle(expression(paste("Pillai's Test for ",tau[1],"=",tau[2],"=0", sep="")))+
#  scale_colour_gradient(breaks=c(0,.02,.04,.06,.08,.1))
  scale_colour_manual(breaks=c(0,.02,.04,.06,.08,.1),
  values = c("grey10", "grey20", "grey30", "grey40", "grey50", "grey60"))

p2 <- qplot(g,T1ErrorRateBeta1, data=errorsummary2, geom="line", group=d, 
      color=d, size=I(.7)) + geom_hline(yintercept=alpha)+
  xlab("g") + theme_bw() + ylim(c(0,0.75)) + 
  theme(legend.position="none") + scale_x_continuous(breaks=c(0,.02,.04,.06,.08,.1))+
  ylab("Type 1 Error Rate")+
  ggtitle(expression(paste("t-test for ",tau[1],"=0", sep="")))+
#  scale_colour_gradient(breaks=c(0,.02,.04,.06,.08,.1))
  scale_colour_manual(breaks=c(0,.02,.04,.06,.08,.1),
  values = c("grey10", "grey20", "grey30", "grey40", "grey50", "grey60"))

p3 <- qplot(g,T1ErrorRateBeta2, data=errorsummary2, geom="line", group=d, 
      color=d, size=I(.7)) + geom_hline(yintercept=alpha)+
  xlab("g") + theme_bw() + ylim(c(0,0.75)) + 
  theme(legend.position="none")+ scale_x_continuous(breaks=c(0,.02,.04,.06,.08,.1))+
  ylab("")+
  ggtitle(expression(paste("t-test for ",tau[2],"=0", sep="")))+
#  scale_colour_gradient(breaks=c(0,.02,.04,.06,.08,.1))
  scale_colour_manual(breaks=c(0,.02,.04,.06,.08,.1),
  values = c("grey10", "grey20", "grey30", "grey40", "grey50", "grey60"))

grid.arrange(p2,p3,p1, nrow=1, widths=c(1,1,1.5))
@

\km{Note that the magnitude of variance scaling parameters is necessarily attributed to non-curricular factors because the generative model is designed to carry no curriculum treatment effect. Great care was taken during the study design and administration to minimize all non-curricular differences that students encountered in lecture and lab sections; using the alternation of instruction, identical curricula administered with all students in the same room in weeks 1 to 8, and careful pedagogical preparation. However, the simulation study indicates that even in the case of very minor lecture or lab based variance structure the model suffers highly inflated Type 1 error rates and gives rise to major doubts about the results of tests for curricular effects discussed in Sections~\ref{ArtistModel} and~\ref{AppliedModel}.}

\km{
%---------------------------------------------------------------------------
\section{Discussion and Conclusions}
\label{discussion}

The ARTIST model analysis indicates that students receiving the simulation-based curriculum have significantly higher learning outcomes for confidence interval related topics. The magnitude of the improvement was 7.146\% on the ARTIST scale, after accounting for the midterm and lab 5 scores. There was no significant difference between traditional and simulation-based curricula on learning outcomes for hypothesis test topics. There are however several issues that merit serious concern and consideration about the validity of these results; including the model assumption of between student independence, the diverse population of interest, the replicability of treatments, and the measurement of learning outcomes. }

\km{The experimental design used randomization of individual students to curricula to aid in the creation of homogeneous groups receiving each the curricular treatment, but the treatments were administered on the lecture and lab section level. The bivariate MANCOVA model was fit under the assumption of independence between students because the lack of repetition in lecture and lab sections does not allow for proper estimation of lecture or lab based variance structure. Even when great efforts are made to control for non-curricular differences in the student experiences, as was done in this study, it is unreasonable to assume that students of the same lecture and/or lab sections would not share some degree of non-curricular connection in learning outcomes as a result of having shared the same physical learning environment. The simulation study indicates that even minor violations of this assumption leads to unacceptable inflation to Type 1 error rates in tests for curriculum effects. }

\km{This error rate inflation is a major problem not only for this study, but every educational study where curricular treatments are implemented in groups of students but comparisons are made between learning measurements from individual students. Alternative experimental designs could implement curricular treatments on the individual student level or have sufficient group replication to make comparisons on the group level; each option suffering major logistical and resource demands. To overcome this issue, a deeper understanding of inter-student variability is needed to support the use of more appropriate covariance structures. Uniformity trials are an established method within agricultural statistics that examines variance structure through experiments with only a single treatment (see e.g.~\citealt{richter2012geostatistical}). If applied in a classroom setting, this approach provides a potential avenue to identify variance structures that form due to student cohorts, which could then be used as a basis of plug-in estimates for variance components in studies without cohort repetition. This would require a widespread and concerted effort by many in the discipline to collect and catalog data on inter-student variance structure from many classroom scenarios. }

We must also bear in mind the population for which the results of this study may be representative. The study was conducted with undergraduate students enrolled in an introductory statistics course at a large public Midwestern university.  The course is required for students in agricultural and biological sciences.  Students in the course are predominantly sophomores and juniors. The results are therefore only applicable to the extent to which these students represent the broader population of introductory statistics students.

% The experimental design bolsters the establishment of a causal effect through utilization of control of non-inferential course components and random assignment of students to treatment groups.  There are two assumptions that we must make to justify a claim of causality.  We must assume that the random assignment successfully eliminated all possible lurking variables (i.e. student demographic and educational backgrounds); however this is the assumption made by most randomized experiments. We must also make the assumption that the instructor effect on learning outcomes has been eliminated by the weekly alternation of instructors. We believe both assumptions are justifiable due to the care taken with randomization and instruction alternation.  

%An issue that is more problematic than the assumptions made about causality is that 
\km{Another important consideration is that the treatment itself was a half semester curriculum -- a highly complex combination of lesson plans, lecture content, assignments and technology use. The treatment complexity poses a problem in identifying precisely which, if any, components of the curriculum might improve learning outcomes. Investigation of the efficacy of the individual components from the improved curriculum is an area for future research. 
%The simulation-based approach to inference that we employed achieved a significant improvement in measured learning outcomes related to confidence intervals, but 
There are also many possible ways to implement simulation-based inference within a course. One noteworthy characteristic was that the simulation-based curriculum that was employed in this study utilized bootstrapping to teach the concepts of confidence intervals as opposed to inverting a simulation-based hypothesis test. This study does not attempt to identify if all possible implementations of the simulation-based approach would achieve improvements in learning outcomes. }

\km{A surprising aspect of the results is that the curriculum effect was more pronounced for the learning outcomes of confidence interval topics than for hypothesis testing topics. It is surprising because much of the literature on the simulation-based approach is focused on the theoretical benefits in simplifying the concepts of hypothesis testing. A potential explanation is that the benefits of the simulation-based methods were counterbalanced by the challenge faced when students were also required to learn theory-based methods; forcing them to mentally reconcile the differences between how each approach obtains a p-value. It is important to recall that due to departmental requirements for the course, the treatment group learned simulation-based inference in addition to an abbreviated unit on theory-based inference methods. However, based on the highly inflated Type 1 error rates in studies with cohort based variance structure, it is also very plausible that the disparity seen is purely random. }

%The simulation-based curriculum lead to no significant difference in learning outcomes for hypothesis testing on the ARTIST scale, despite the added complexity of learning additional concepts for conducting simulation-based tests. In addition, the added complexity related to bootstrapping confidence intervals appears to have actually improved learning outcomes for confidence intervals on the ARTIST scale. The transition between approaches may have simply been easier for confidence interval topics; the primary difference being that the bootstrap estimated standard error was replaced by a simple theory-based estimator of the standard error. 

The evidence of improved learning outcomes is also contingent on the efficacy of the ARTIST scales to measure student learning. The ARTIST question sets have been criticized as being increasingly outdated and for lacking reliability and validity evidence~\citep{Ziegler2014}. The Comprehensive Assessment of Outcomes in Statistics (CAOS; \citealt{DelMas2007}) was considered as alternative assessment of student learning because it is nationally normed and backed by a reliability study; however, the reliability was assessed for the CAOS test in its entirety (40 items) which was decided to be too extensive to be administered in addition to the other necessary components on the final exam. The Reasoning about P-values and Statistical Significance (RPASS; \citealt{LaneGetaz2013}) assessment was also considered, but not selected, because it does not assess learning outcomes for confidence interval related topics. The Basic Literacy in Statistics (BLIS; \citealt{Ziegler2014}) and the Goals and Outcomes Associated with Learning Statistics (GOALS; \citealt{Garfield2012}) assessments were recently designed to better measure student learning in the contemporary statistics classroom, unfortunately these assessments were in development at the time of this study.

\km{ This study is unable to draw reliable conclusions on the efficacy of simulation-based methods for teaching statistical inference due to the volatility of the Type 1 error rates under minor violations to model assumptions. This is a fundamental problem for all comparative educational studies where pedagogical treatments are administered on the class level and measurements are taken the individual student level. Viewed as a case study of two curricula administered within extremely similar student groups, we found that simulation-based curriculum had a noticeable improvement in learning outcomes associated with confidence intervals and a small improvement in learning outcomes associated with hypothesis testing. While the results are clearly not conclusive in assessing the curricular effect on learning outcomes, the findings do merit further consideration into the pedagogical benefits of a simulation-based curriculum. }

\newpage

%appendix-------------------------------------------------------

\section{Appendix: Final Exam Used in Curricula Study}
\label{appendA}

The following appendix contains the questions and point rubrics for the ARTIST scaled questions and the applied theory-based inference problems as they appeared on the final exam.

\subsection{ARTIST Scaled Multiple Choice Question Set for Confidence Intervals}
\label{appendA1}

1. Answer the following general multiple choice questions regarding confidence intervals.  There is only one correct answer for each (circle the best option).

i. Two different samples will be taken from the same population of test scores where the population mean and standard deviation are unknown. The first sample will have 25 data values, and the second sample will have 64 data values. A 95\% confidence interval will be constructed for each sample to estimate the population mean. Which confidence interval would you expect to have greater precision (a smaller width) for estimating the population mean?\\
a. I expect the confidence interval based on the sample of 64 data values to be more precise.\\
b. I expect both confidence intervals to have the same precision.\\
c. I expect the confidence interval based on the sample of 25 data values to be more precise.\\

ii. A 95\% confidence interval is computed to estimate the mean household income for a city. Which of the following values will definitely be within the limits of this confidence interval?\\
a. The population mean\\
b. The sample mean\\
c. The standard deviation of the sample mean\\
d. None of the above\\

iii. Each of the 110 students in a statistics class selects a different random sample of 35 Quiz scores from a population of 5000 scores they are given. Using their data, each student constructs a 90\% confidence interval for $\mu$ the average Quiz score of the
5000 students. Which of the following conclusions is correct?\\
a. About 10\% of the sample means will not be included in the confidence intervals.\\
b. About 90\% of the confidence intervals will contain $\mu$.\\
c. It is probable that 90\% of the confidence intervals will be identical.\\
d. About 10\% of the raw scores in the samples will not be found in these confidence intervals\\

iv. A 95\% confidence interval for the mean reading achievement score for a population of third grade students is (43, 49). The margin of error of this interval is:\\
a. 5\\
b. 3\\
c. 6\\

v. Justin and Hayley conducted a mission to a new planet, Planet X, to study arm length. They took a random sample of 100 Planet X residents and calculated a 95\% confidence interval for the mean arm length. What does a 95\% confidence interval for arm length tell us in this case? Select the best answer:\\
a. I am 95\% confident that this interval includes the sample mean (x) arm length.\\
b. I am confident that most (95\%) of all Planet X residents will have an arm length within this interval.\\
c. I am 95\% confident that most Planet X residents will have arm lengths within this interval.\\
d. I am 95\% confident that this interval includes the population mean arm length.\\

vi. Suppose that a random sample of 41 state college students is asked to measure the length of their right foot in centimeters. A 95\% confidence interval for the mean foot length for students at this university turns out to be (21.709, 25.091). If instead a 90\% confidence interval was calculated, how would it differ from the 95\% confidence interval?\\
a. The 90\% confidence interval would be narrower.\\
b. The 90\% confidence interval would be wider.\\
c. The 90\% confidence interval would be the same as the 95\% confidence interval.\\

vii. A pollster took a random sample of 100 students from a large university and computed a confidence interval to estimate the percentage of students who were planning to vote in the upcoming election. The pollster felt that the confidence interval was too wide to provide a precise estimate of the population parameter. What could the pollster have done to produce a narrower confidence interval that would produce a more precise estimate of the percentage of all university students who plan to vote in the upcoming election?\\
a. Increase the sample size to 150.\\
b. Increase the confidence level to 99\%.\\
c. Both a and b\\
d. None of the above\\

viii. A newspaper article states with 95\% confidence that 55\% to 65\% of all high school students in the United States claim that they could get a hand gun if they wanted one. This confidence interval is based on a poll of 2000 high school students in Detroit. How would you interpret the confidence interval from this newspaper article?\\
a. 95\% of large urban cities in the United States have 55\% to 65\% high school students who could get a hand gun.\\
b. If we took many samples of high school students from different urban cities, 95\% of the samples would have between 55\% and 65\% high school students who could get hand guns.\\
c. You cannot use this confidence interval to generalize to all teenagers in the United States because of the way the sample was taken.\\
d. We can be 95\% confident that between 55\% and 65\% of all United States high school students could get a hand gun.\\

ix. The Gallup poll (August 23, 2002) reported that 53\% of Americans said they would favor sending American ground troops to the Persian Gulf area in an attempt to remove Hussein from power. The poll also reported that the margin of error for this poll was 4\%. What does the margin of error of 4\% indicate?\\
a. There is a 4\% chance that the estimate of 53\% is wrong.\\
b. The percent of Americans who are in favor is probably higher than 53\% and closer to 57\%.\\
c. The percent of Americans who are in favor is estimated to be between 49\% and 57\%.\\

x. Suppose two researchers want to estimate the proportion of American college students who favor abolishing the penny. They both want to have about the same margin of error to estimate this proportion. However, Researcher 1 wants to estimate with 99\% confidence and Researcher 2 wants to estimate with 95\% confidence. Which researcher would need more students for her study in order to obtain the desired margin of error?\\
a. Researcher 1.\\
b. Researcher 2.\\
c. Both researchers would need the same number of subjects.\\
d. It is impossible to obtain the same margin of error with the two different confidence levels.\\

\subsection{ARTIST Scaled Multiple Choice Question Set for Hypothesis Testing}

2.   Answer the following general multiple choice questions regarding hypothesis testing.  There is only one correct answer for each (circle the best option).  As a helpful note the term ``statistically significant'' means that you reject the null hypothesis.

i. The makers of Mini-Oats cereal have an automated packaging machine that is set to fill boxes with 24 ounces of cereal. At various times in the packaging process, a random sample of\\

\noindent 100 boxes is taken to see if the machine is filling the boxes with an average of 24 ounces of cereal. Which of the following is a statement of the null hypothesis being tested?\\
a. The machine is filling the boxes with the proper amount of cereal.\\
b. The machine is not filling the boxes with the proper amount of cereal.\\
c. The machine is not putting enough cereal in the boxes.\\

ii. A research article gives a p-value of .001 in the analysis section. Which definition of a p-value is the most accurate?\\
a. the probability that the observed outcome will occur again.\\
b. the probability of observing an outcome as extreme or more extreme than the one observed if the null hypothesis is true.\\
c. the value that an observed outcome must reach in order to be considered significant under the null hypothesis.\\
d. the probability that the null hypothesis is true.\\

iii. If a researcher was hoping to show that the results of an experiment were statistically significant they would prefer:\\
a. a large p-value\\
b. a small p-value\\
c. p-values are not related to statistical significance\\


iv. A researcher compares men and women on 100 different variables using a difference in means t-test.  He sets the level of significance at 0.05 and then carries out 100 independent t-tests (one for each variable) on these data.  If, for each test, the null hypothesis is actually true, about how many ``statistically significant'' results will be produced?\\
a. 0\\
b. 5\\
c. 10\\
d. none of the above\\

Problems (v) and (vi) refer to the following situation: Food inspectors inspect samples of food products to see if they are safe. This can be thought of as a hypothesis test where Null: the food is safe, and Alternative: the food is not safe. Identify each of the following statements as a Type I or a Type II error.\\

v. The inspector says the food is safe but it actually is not safe.\\
a. Type I\\
b. Type II\\

vi. The inspector says the food is not safe but is actually safe.\\
a. Type I\\
b. Type II\\

vii. A newspaper article claims that the average age for people who receive food stamps is 40 years. You believe that the average age is less than that. You take a random sample of 100 people who receive food stamps, and find their average age to be 39.2 years. You find that this is significantly lower than the age of 40 stated in the article (p-value < .05). What would be an appropriate interpretation of this result?\\
a. The statistically significant result indicates that the majority of people who receive food stamps is younger than 40.\\
b. Although the result is statistically significant, the difference in age is not of practical importance.\\
c. An error must have been made. This difference is too small to be statistically significant.\\

viii. A newspaper article stated that the US Supreme Court received 812 letters from around the country on the subject of whether to ban cameras from the courtroom. Of these 812 letters, 800 expressed the opinion that cameras should be banned. A statistics student was going to use this sample information to conduct a test of significance of whether more than 95\% of all American adults feel that cameras should be banned from the courtroom. What would you tell this student?\\
a. This is a large enough sample to provide an accurate estimate of the American public's opinion on the issue.\\
b. The necessary conditions for a test of significance are not satisfied, so no statistical test should be performed.\\
c. With such a large number of people favoring the notion that cameras be banned, there is no need for a statistical test.\\

ix. A researcher conducts an experiment on human memory and recruits 15 people to participate in her study. She performs the experiment and analyzes the results. She obtains a p-value of .17. Which of the following is a reasonable interpretation of her results?\\
a. This proves that her experimental treatment has no effect on memory.\\
b. There could be a treatment effect, but the sample size was too small to detect it.\\
c. She should reject the null hypothesis.\\
d. There is evidence of a small effect on memory by her experimental treatment.\\

x. It is reported that scores on a particular test of historical trivia given to high school students are approximately normally distributed with a mean of 85. Mrs. Rose believes that her 5 classes of high school seniors will score significantly better than the national average on this test. At the end of the semester, Mrs. Rose administers the historical trivia test to her students. The students score an average of 89 on this test. After conducting the appropriate statistical test, Mrs. Rose finds that the p-value is .0025. Which of the following is the best interpretation of the p-value?\\
a. A p-value of .0025 provides strong evidence that Mrs. Rose's class outperformed high school students across the nation.\\
b. A p-value of .0025 indicates that there is a very small chance that Mrs. Rose's class outperformed high school students across the nation.\\
c. A p-value of .0025 provides evidence that Mrs. Rose is an exceptional teacher who was able to prepare her students well for this national test.\\
d. None of the above.\\


\subsection{Applied Theory-Based Confidence Interval Question}

3. Farmer Cindy is in charge of the chickens on her family's farm, and is curious about the average number of eggs the entire flock produces in a month.  Observing the entire flock would be time consuming, so she approaches you asking what her options are. You inform her that 30 chickens should be selected at random to be observed for a month. After the month she observes the average number of eggs her sample of 30 chickens produced is 25 eggs with a standard deviation of 6 eggs.  
\begin{enumerate} 
\item (5 pts) Construct a 95\% confidence interval for the average number of eggs chickens from her entire population produce in a month.  (You don't need to check any conditions here)  
\item (3 pts) Interpret the 95\% confidence interval constructed in the previous part:
\item (2 pts) Cindy is disappointed with the width of the interval you provide for her, suggest to her two ways she could obtain a narrower confidence interval.
\end{enumerate}

\subsection{Applied Theory-Based Hypothesis Testing Question}

4. Cindy becomes concerned about a disease some of her chickens are catching that causes a decrease in the chicken's egg production.  She is interested in the proportion of her entire flock that has the disease, but detecting the disease requires taking blood from the chicken which is expensive and time consuming.  After working with you in the past, she understands that she can estimate this proportion by taking just a sample of her chickens! Assume she selects a sample of 100 chickens in the best possible way and observed that 15 of the chickens had the disease.

Cindy's pessimistic guess is that 25\% of the flock has the disease.  Complete the following steps to test if the proportion of her entire population diseased is less than 0.25.\\
\textbf{(a)} [2 points] State the Null and Alternative hypothesis:\\
\textbf{(b)} [2 points] Check the conditions for a hypothesis test:\\
\textbf{(c)} [2 points] Calculate the test statistic:\\
\textbf{(d)} [2 points] Find the p-value:\\
\textbf{(e)} [3 points] Make a decision about your hypothesis and state your conclusion in context of the problem.
%-------------------------------------------------------
\section{Appendix: Midterm Exam Used in Curricula Study}
\label{appMidtermExamQuestions}

The following appendix is the midterm exam in its entirety, along with point designations for each question.  Note that Problem 6 for the midterm is the ARTIST scaled question set for assessing topics related to "data collection". This ARTIST set was included with the goal to strengthen the midterm as a effective covariate for controlling for pre-treatment differences in the curricula groups in the model comparing learning outcomes using the ARTIST scores. 

<<configForMidtermAppendix, echo=FALSE, include=FALSE >>=
### Setting up R ###
#load necessary packages
#library(productplots)
library(graphics)
library(ggplot2)
library(gridExtra)
library(reshape)
library(reshape2)
library(plyr)

#Wellbeing metrics
wbmetric <- "thriving" ; wbperc <- 55

#diamonds data for regression problem
set.seed(12)
index <- sample(1:nrow(diamonds),150)
diamondsamp <- diamonds[index,c(1,7)]
head(diamondsamp)

# smoking and mortality data from DASL 
# http://lib.stat.cmu.edu/DASL/Stories/SmokingandCancer.html
  # smoking = rate of smoking in percentage of average male smoking
  # Mortality = rate of lung cancer mortality in percentage of average male lung cancer mortality
smokedie <- read.delim("smokedie.txt",header=T, )

#Census data for sidebyside boxplot problem
censussamp <- read.csv("CensusDataForExams.csv",header=T)
censussamp$incinthous <- censussamp$inctot/1000
censussamp$gender <- "Female" ; censussamp$gender[censussamp$sex==1] <- "Male"
cenmale <- summary(censussamp[censussamp$gender=="Male", ]$inctot)
cenfemale <- summary(censussamp[censussamp$gender=="Female", ]$inctot)

#marraige role for problem 2
mr2 <- "wife"
mr1 <- "husband"

#iris data for side by side boxplot problem
liliris <- iris[iris$Species!="setosa",c(3,5)] #versionA
#liliris <- iris[iris$Species!="virginica",c(3,5)] #versionB
flower1 <- "Virginica" #VersionA
#flower1 <- "Setosa" #VersionB
flower2 <- "Versicolor"

# Marble Example for Discrete RV Problem
colornum <- c(10,8,3,2)
colorpts <- c(2,5,10,15)
totmarb <- sum(colornum)

# Housing data for Univariate Numerical Variable Problem
load("realestate.rda")
realestate <- realestate[1:nrow(realestate), ]
#Version A
xl <- "Home Price (dollars)" ; yl <- "Count" 
maintit <-"Histogram of Ames Home Prices"  ; homeattr <- "price"
plotattr <- realestate$price ; binw <- 40000

#Version B
#xl <- "Home Floorspace (Square Feet)" ; yl <- "Count" 
#maintit <-"Histogram of Ames Home Floorspaces" ; homeattr <- "floorspace in square feet"
#plotattr <- realestate$sqft ; binw <- 300

#nturns <- 11
nturns <- 12
@

%---------------------------------------------------------------------

\textbf{1. [28 points] Linear Regression:} A study was conducted in England in 1989 investigating the relationship between smoking and lung cancer mortality within different occupational groups of males. The data include a smoking index and a lung cancer mortality index for men in \Sexpr{nrow(smokedie)} occupational groups in England.

The smoking index measures the percentage of the number of cigarettes smoked per day by men in the particular occupational group compared to the average number of cigarettes smoked per day by all men. The units associated with the smoking index are percent of average cigarettes smoked (\% avg CS).  The mortality index measures the percentage of the rate of deaths from lung cancer among men in the particular occupational group compared to the rate of deaths from lung cancer among all men. The units associated with the mortality index are percent of average lung cancer deaths (\% avg LCD).  

The data for the \Sexpr{nrow(smokedie)} occupations are displayed in the scatterplot below. Use the information from the scatterplot, the sample statistics and linear regression equation listed below to answer all parts of this problem. \\

<< 'regprobplot' , echo=FALSE, warning=FALSE, fig.width=6, fig.height=6, out.width='.5\\linewidth', fig.pos='h',fig.align='center'>>=
mod1 <- lm(Mortality ~ Smoking, smokedie)
#str(summary(mod1))
int <- mod1$coefficients[1]
slo <- mod1$coefficients[2]


qplot(Smoking,Mortality, geom="point", data=smokedie) + geom_abline(intercept = int, slope = slo, size=I(1), colour=I("black"))+ ylab("Mortality (% avg LCD)") + xlab("Smoking (% avg CS)") + ggtitle("Scatterplot of Mortality Index by Smoking Index with Fitted Regression Line") + theme_bw()

ybar1 <- round(mean(smokedie$Mortality),digits=4)
xbar1 <- round(mean(smokedie$Smoking),digits=4)
sy1 <- round(sd(smokedie$Mortality),digits=4)
sx1 <- round(sd(smokedie$Smoking),digits=4)
rsq1 <- round(summary(mod1)$r.squared, 4)-0.0001
r1 <- round(sqrt(rsq1),4)
@
\begin{table}[hbtp]\centering 
\begin{tabular}{ |c|c|c|}
  \hline                        
 $\bar{x}$ = \Sexpr{xbar1}\% avg CS & $\bar{y}$ = \Sexpr{ybar1}\% avg LCD & r=\Sexpr{r1}\\
 $S_x$ = \Sexpr{sx1} \% avg CS & $S_y$ = \Sexpr{sy1}\% avg LCD & $R^2$ = \Sexpr{rsq1}\\
  \hline  
\end{tabular}
\end{table}

\begin{center}
\textbf{Prediction Equation:} \hspace{.2in}  $\hat{y} = \Sexpr{int} + x(\Sexpr{slo})$
\end{center}
\noindent\textbf{(a)} [2 points] What is the response variable?\\
\noindent\textbf{(b)} [2 points] What is the explanatory variable?\\
\noindent\textbf{(c)} [4 points] The correlation coefficient, r=\Sexpr{r1}.  Using this value, what can be said about the relationship between the smoking index and the lung cancer mortality index?\\
\noindent\textbf{(d)} [4 points] The slope of the linear regression model is \Sexpr{slo}. Show the calculations to prove that this is the value for the slope. Then, interpret this value in the context of the problem.\\
\noindent\textbf{(e)} [4 points] The intercept of the linear regression model is \Sexpr{int}. First, show the calculations to prove that this is the value for the intercept. If it is appropriate to interpret this value in the context of the problem, then do so. Otherwise, specify why it is inappropriate to interpret this value.\\
\noindent\textbf{(f)} [3 points] In the prediction equation for the regression line explain that the symbols $\hat{y}$ and x represent.\\
\noindent\textbf{(g)} [3 points] Using the prediction line given above, predict the morality index (in \% avg LCD) for an occupational group that has a smoking index of 110\% avg CS (round your answer to 2 decimal places).\\
\noindent\textbf{(h)} [3 points] Suppose that there was an occupational group in our data set with a smoking index of 110\% avg CS and a mortality index of 140\% avg LCD.  Calculate the residual for this occupational group (round your answer to 2 decimal places).\\
\noindent\textbf{(i)} [3 points] Interpret the $R^2$ value in terms of the context of the problem.\\
%--------------------------------------------------------------------------

\textbf{2. [10 points]} A Gallup poll conducted on February 26, 2014 investigated the wellbeing of American adults. The survey used the Gallup-Healthways Well-Being Index to classify the person's wellbeing as "thriving", "struggling" or "suffering". The telephone survey found that \Sexpr{wbperc}\% of 1500 randomly selected American adults were classified as \Sexpr{wbmetric}.  \\
For this study, identify the following:\\
\noindent\textbf{(a)} [2 points] Identify the population of interest.\\
\noindent\textbf{(b)} [2 points] Identify the sample.\\
\noindent\textbf{(c)} [2 points] Identify the population parameter.\\
\noindent\textbf{(d)} [2 points] Identify the sample statistic.\\
\noindent\textbf{(e)} [2 points] Identify the variable.\\

%--------------------------------------------------------------------------

\textbf{3. [10  points]} A study investigated the relationship between heights of husbands and wives. Heights were recorded as groupings of "Tall", "Medium" and "Short". The researcher recorded height pairings for 205 married couples and wants to know how the height of the husband associated the height of the wife. Use the information found in the contingency table below to answer the questions that follow.
\begin{table}[hbtp]\centering 
\begin{tabular}{ |cc|ccc|c|}
  \hline                        
  & & & Wife & & \\
  & & Tall & Medium & Short & Total \\
 \hline
          & Tall    & 18 & 28  & 14  & 60  \\
Husband   & Medium  & 20 & 51  & 28  & 99  \\
          & Short   & 12 & 25  & 9   &  46 \\
  \hline  
  & Total & 50 & 104 & 51 & 205 \\
  \hline
\end{tabular}
\end{table}

\noindent\textbf{(a)} [2  points] What is the probability that a randomly selected married couple will have a short \Sexpr{mr1}?\\
\noindent\textbf{(b)} [2  points] What is the probability that a randomly selected married couple will have a tall \Sexpr{mr2} \textit{and} a short \Sexpr{mr1}?\\
\noindent\textbf{(c)} [2  points] What is the probability that a randomly selected married couple will have a tall \Sexpr{mr2} \textit{or} a short \Sexpr{mr1}?\\
\noindent\textbf{(d)} [2  points] What is the probability that a randomly selected married couple will have a short \Sexpr{mr1} \textit{given} that we know the \Sexpr{mr2} is tall?\\
\noindent\textbf{(e)} [2  points] Are the events of short \Sexpr{mr1} and tall \Sexpr{mr2} \textit{independent}? Justify your answer mathematically.\\

%--------------------------------------------------------------------------

\textbf{4. [8 points]} Suppose that we have a box containing \Sexpr{totmarb} marbles of equal size and shape so that each marble is equally likely to be selected from the box.  There are \Sexpr{colornum[1]} red, \Sexpr{colornum[2]} green, \Sexpr{colornum[3]} yellow marbles and  \Sexpr{colornum[4]} black marbles.  Suppose we assign points to each color marble: \Sexpr{colorpts[1]} points for a red, \Sexpr{colorpts[2]} point for a green, \Sexpr{colorpts[3]} points for a yellow and \Sexpr{colorpts[4]} points for a black. Let X be the discrete random variable that counts the number of points that a randomly selected marble is worth.\\
\noindent\textbf{(a)} [2 points] Find the probability distribution of X? (fill out table and round all values to 2 decimal places)\\
\begin{table}[hbtp]\centering 
\begin{tabular}{ c|c }
x & P(x) \\
  \hline  
   &  \\
   &  \\
   &  \\  
\end{tabular}
\end{table}

\noindent\textbf{(b)} [4 points] What 2 properties must be true for the values in the P(x) column of the probability distribution in part 4(a)? Note that X is a discrete random variable.\\
\noindent\textbf{(c)} [2 points] What is the mean of X? (If you were unable to complete part 4(a), describe how you would find the mean) \\

%---------------------------------------------------------------------

\textbf{5. [10 points]} A survey on gender and income was conducted and a random sample of 3700 Adult U.S. residents was gathered. In the figure below are side-by-side boxplots of the income (in thousands of dollars) for male and female U.S. residents. Use this graphical display to answer the following questions.
<< 'sidebysidebox' , echo=FALSE, warning=FALSE, fig.width=9, fig.height=4.5, out.width='.8\\linewidth', fig.pos='H',fig.align='center'>>=
qplot(factor(gender), incinthous, data = censussamp, geom = "boxplot") + coord_flip() + xlab("Gender") + ylab("Income (in thousands of dollars)") + ggtitle("Side by Side Boxplots for Incomes") + theme_bw()
@
\begin{table}[H]\centering 
\begin{tabular}{ |c||c|c||c|c|c|c|c|}
  \hline                        
Gender  & mean & sd & min & Q1 & median & Q3 & max \\
 \hline
 Male & 39319 & 54242 & -10000 & 12000 & 25600 & 46700 & 685000 \\
 Female & 18786 & 21570 & -10000 & 4880 & 13500 & 25800 & 239000 \\
  \hline
\end{tabular}
\end{table}
\noindent\textbf{(a)} [5 points] Using full sentences, describe the distribution of income for \textit{only} the male residents of the U.S.  \\
\noindent\textbf{(b)} [5 points] Using full sentences, compare the distribution of male and female distributions of income. A complete answer will compare shapes, centers and spreads. Be specific about which measures of center and spread are being used to make these comparisons. (ie writing "the spread is bigger" will not earn full credit)  \\

%--------------------------------------------------------------------------

\underline{\textbf{6. [18 points]} Multiple Choice: \textit{Clearly} circle the selected answer.}\\
\noindent\textbf{(i)} [2 points] In a survey people are asked 'Which brand of toothpaste do you prefer?' The data gathered from this question would be what type of data? \\
\indent a. continuous \\
\indent b. categorical \\
\indent c. quantitative 

\textbf{Items ii and iii refer to the following situation:}
A student is gathering data on the driving experiences of other college students. One of the variables measured is the type of car the student drives. These data are coded using the following method: 1 = subcompact, 2 = compact, 3 = standard size, 4 = full size, 5 = premium, 6 = mini van, 7 = SUV, and 8 = truck.

\noindent\textbf{(ii)} [2 points] What type of variable is this? \\
\indent  a. categorical \\
\indent  b. quantitative \\
\indent  c. continuous 

\noindent\textbf{(iii)} [2 points] The student plans to see if there is a relationship between the number of speeding tickets a student gets in a year and the type of vehicle he or she drives. Identify the response variable in this study. \\
\indent a. college students \\
\indent b. type of car \\
\indent c. number of speeding tickets \\
\indent d. average number of speeding tickets last year 

\noindent\textbf{(iv)} [2 points] A researcher is studying the relationship between a vitamin supplement and cholesterol level. What type of study needs to be done in order to establish that the amount of vitamin supplement causes a change in cholesterol level? \\
\indent a. Survey \\
\indent b. Randomized experiment \\
\indent c. Time Series Study \\
\indent d. Survey 

\noindent\textbf{(v)} [2 points] An instructor is going to model an experiment in his statistics class by comparing the effect of 4 different treatments on student responses. There are 40 students in the class.
is the best way for the instructor to distribute the students to the 4 treatments for this experiment? \\
\indent a. Assign the first treatment to the first 10 students on the class list, the second treatment to the next 10 students, and so on. \\
\indent b. Assign a unique number to each student, then use random numbers to assign 10 students to the first treatment, 10 students to the second treatment, and so on. \\
\indent c. Assign the treatment as students walk into class, giving the first treatment to the first 10 students and the second treatment to the next 10 student, and so on. \\
\indent d. All of these are equally appropriate methods. \\
\indent e. None of these is an appropriate method.

\textbf{Items vi and vii refer to the following situation:}\\
Suppose two researchers wanted to determine if aspirin reduces the chance of a heart attack.

\noindent\textbf{(vi)} [2 points] Researcher 1 studied the medical records of 500 randomly selected patients. For each patient, he recorded whether the person took aspirin every day and if the person had ever had a heart attack. Then he reported the percentage of heart attacks for the patients who took aspirin every day and for those who did not take aspirin every day. What type of study did Researcher 1 conduct? \\ 
\indent a. Observational \\
\indent b. Experimental \\
\indent c. Survey \\
\indent d. None of the above

\noindent\textbf{(vii)} [2 points] Researcher 2 also studied 500 patients that visited a regional hospital in the last year. He randomly assigned half (250) of the patients to take aspirin every day and the other half to take a placebo everyday. Then after a certain length of time he reported the percentage of heart attacks for the patients who took aspirin every day and for those who did not take aspirin every day. What type of study did Researcher 2 conduct? \\ 
\indent a. Observational \\
\indent b. Experimental \\
\indent c. Survey \\
\indent d. None of the above 

\noindent\textbf{(vii)} [2 points] The dean of a college would like to determine the feelings of students concerning a new registration fee that would be used to upgrade the recreational facilities on campus. All registered students would pay the fee each term. Which of the following data collection plans would provide the best representation of students' opinions at the school? \\
\indent a. Survey every 10th student who enters the current recreational facilities between the hours of 1:00 and 5:00 pm until 100 students have been asked. \\
\indent b. Randomly sample fifty student ID numbers and send a survey to all students in the sample. \\
\indent c. Place an ad in the campus newspaper inviting students to complete an online survey. Collect the responses of the first 200 students who respond. \\
\indent d. All of the above would be equally effective. 

\noindent\textbf{(ix)} [2 points] A team in the Department of Institutional Review at a large university wanted to study the relationship between completing an internship during college and students' future earning potential. From the same graduating class, they selected a random sample of 80 students who completed an internship and 100 students who did not complete an internship and examined their salaries 5 years past graduation. They found that there was a statistically higher mean salary for the internship group than for the non-internship group. Which of the following interpretations do you think is the most appropriate?\\
\indent a. More students should take internships because having an internship produces a higher salary.\\
\indent b. There could be a confounding variable, such as student major, that explains the difference in mean salary between the internship and no internship groups. \\
\indent c. You cannot draw any valid conclusions because the samples are not the same size.\\

%--------------------------------------------------------------------------

\textbf{7. [8 points]} When playing the game Settlers of Catan\textbf{TM} there is a game piece known as the robber that blocks resources from being obtained.  On each player's turn, that player rolls a pair of 6 sided dice.  If the sum of the dice roll equals 7 then that player may move the robber. This means that there is a 6/36 chance of moving the robber with every roll of the dice, because there are 6 of the 36 possible combinations that sum to 7. Let X count the number of times that one player moves the robber in \Sexpr{nturns} turns.\\
\noindent\textbf{(a)} [4 points]  Check the 4 conditions to argue that X is a Binomial Random Variable  \\
\noindent\textbf{(b)} [2 points]  What is the probability that a player gets to move the robber 2 times during the \Sexpr{nturns} turns?  \\
\noindent\textbf{(c)} [2 points]  What is the mean number of times during \Sexpr{nturns} turns that a player will be able to move the robber?\\

%-----------------------------------------------------------------------

\textbf{8. [4 points]} A waiter from a restaurant keeps track of the information about tips he receives from dinning parties. Part of this data includes the tipping rate, as a proportion of the total bill, that was tipped. Below is a histogram of tipping rates from the 244 dinning parties served by the waiter. Use this histogram to answer the following question.
<< 'tipratehisto' , echo=FALSE, warning=FALSE, fig.width=9, fig.height=3.5, out.width='.7\\linewidth', fig.pos='H',fig.align='center'>>=
qplot(tip/total_bill, data=tips, binwidth=.04, xlim=c(0,.8)) + 
  xlab("Tipping Rate (proportion of total bill)") + ylab("Count") + ggtitle("Histogram of Tipping Rates") + theme_bw()
@
\noindent\textbf{(a)} [2 points]  What is the best measure of center to describe the distribution of tipping rates? Explain your answer. (Note: no calculations or estimated values are needed to answer this problem)  \\
\noindent\textbf{(b)} [2 points]  What is the best measure of spread to describe the distribution of tipping rates? Explain your answer. (Note: no calculations or estimated values are needed to answer this problem) \\

%-----------------------------------------------------------------------

\textbf{9. [4 points]} Below are histograms for two separate random samples of 30 quiz scores.  Which quiz will have the larger sample standard deviation for quiz scores? \textbf{Explain your choice briefly}. (Note: You do not, and should not, do any calculations to answer this question)
\begin{figure}[H]
\begin{center}
\includegraphics[keepaspectratio=TRUE,width=.2\textwidth]{QuizScores1Version1.png}
\includegraphics[keepaspectratio=TRUE,width=.2\textwidth]{QuizScores2Version1.png}
\end{center}
\end{figure}


%-------------------------------------------------------
\section{Appendix: MANCOVA Model Diagnostics}
\label{appModDiag}

For the ARTIST Model described in Subsection~\ref{ArtistModel} and the Applied Model described in Subsection~\ref{AppliedModel} a set of model diagnostics were conducted. Each of these MANCOVA models are parameterized as specified in Section~\ref{analysis}; as such the assessment of model assumptions will be very similar for each model.   Residual plots will be used to assess the assumptions of linearity and constant variance.  The univariate and bivariate normality of the error terms will be assessed visually using normal quantile plots and a scatterplot of the paired residuals from the model. Additionally, although it is not a modeling assumption, MANCOVA models are best behaved with low to moderate correlation between the response variables, because then then model is able to capture variance unique to each response. 

\subsection{ARTIST Model Diagnostics}
\label{appARTISTModDiag}

The ARTIST Model is assessed to satisfactorily meet the modeling assumptions. The correlation between the ARTIST scores for confidence intervals and hypothesis tests is acceptable for modeling with MANCOVA with a correlation of $\Sexpr{round(with(dat, cor(ConfMC,HypMC)),3)}$ between the responses.  The normality of the errors is upheld by the plots in Figure~\ref{fig:ARTISTModDiag}.  The normal quantile plots only display a slight bend for the residuals from the Hypothesis Test topic scores, and the bivariate distribution of the residuals are visually consistent with bivariate normality. 

<<ARTISTModSelect,echo=F,include=F,eval=T>>=
### Build MANCOVA model to be used elseware in appendix
mod2 <- lm(cbind(ConfMC,HypMC)~ midterm + Section +
             hw1perc +  hw2perc + hw3perc + hw4perc + hw5perc + hw6perc + hw7perc +
            lab1perc +lab2perc +lab3perc +lab4perc +lab5perc +lab6perc + lab7perc +  room , data=dat)
mod2backward <- mStep(mod2, k=2, trace=TRUE) #k=2 means to use AIC
mod2small <- update(mod2backward, .~. -  lab2perc - hw4perc - hw5perc)
summarytab <- summary(manova(mod2small))$stats
mod2smallCI <- lm(ConfMC~ midterm + lab5perc + room, data=dat)
CIcis <- data.frame(confint(mod2smallCI,level=.95))
CIcis$ests <- mod2smallCI$coeff
mod2smallHT <- lm(HypMC~ midterm + lab5perc + room, data=dat)
HTcis <- data.frame(confint(mod2smallHT,level=.95))
HTcis$ests <- mod2smallHT$coeff
@

<<ARTISTModDiag, echo=FALSE , warning=FALSE, fig.width=5, fig.height=5, out.width='.49\\linewidth', fig.pos='h',fig.align='center',fig.cap="Normal quantile plots (left) and bivariate scatterplot (right) for residuals of each response from ARTIST Model.",fig.scap="Normal quantile plots and bivariate scatterplot for residuals of each response from ARTIST Model.",fig.show='hold'>>=
#--------------------------------------------------
### Check Conditions for MANCOVA linear model fit
## check response correlations to avoid colinearity in multivariate model
#Rule of Thumb: ~ .3 to .55 then MANCOVA will work well
#with(dat, cor(ConfMC,HypMC)) #Not overly correlated

### check for influencial and high leverage points
#infmeas <- influence.measures(mod2small)
#head(infmeas$infmat)
#head(infmeas$is.inf)
### no high leverage

#Normality of Residuals
#op <- par(mfrow = c(1, 3))      
#qqp1 <- qqPlot(mod2small$residuals[,1], xlab="Theoretical Normal Quantiles",
#       ylab="ARTIST CI Score Residual")

#qqp2 <- qqPlot(mod2small$residuals[,2], xlab="Theoretical Normal Quantiles",
#       ylab="ARTIST HT Score Residual")
#op

qqp1 <- qplot(sample=mod2small$residuals[,1], stat="qq", xlab="Theoretical Normal Quantiles",
       ylab="ARTIST CI Score Residual") +  theme_bw()

qqp2 <- qplot(sample=mod2small$residuals[,2], stat="qq", xlab="Theoretical Normal Quantiles",
      ylab="ARTIST HT Score Residual") +  theme_bw()

grid.arrange(qqp1,qqp2,nrow=2)

qplot(mod2small$residuals[,1],mod2small$residuals[,2],
      xlab="ARTIST CI Score Residual",  ylab="ARTIST HT Score Residual")  +  theme_bw()
# Univariate and Bivariate normality looks fine

# makeenv <- function(res){
# grav.qqboot <- boot(res,sort,R=999,sim="parametric",ran.gen=grav.gen)
# theo.qq <- qqnorm(res,plot=FALSE)
# theo.qq <- lapply(theo.qq,sort)
# env <- envelope(grav.qqboot,level=0.95)
# envdat <- data.frame(theoquant = c(theo.qq$x,theo.qq$x),
#                      envelope = c(env$point[1,],env$point[2,]),
#                      bound = rep(c("upper","lower"),each=length(res)))
# return(envdat)
# }
# 
# envdat <- makeenv(mod2small$residuals[,1])
# head(envdat)
# qplot(sample = mod2small$residuals[,1], stat = "qq") +
#   geom_path(aes(x=theoquant, y=envelope), data=subset(envdat,bound=="lower")) +
#   geom_path(aes(x=theoquant, y=envelope), data=subset(envdat,bound=="upper"))
  
  
## Independence of responses between students must be assumed
@

The residual plots in Figure~\ref{fig:residualsARTIST} display stripped bands of points that run shallowly downward, creating the optical illusion of a trend in the points.  This is because the model fits the discretely recorded ARTIST scores as continuous response variables, thus there is a discrete set of residuals possible for any particular fitted value.  Note that, as with all least-squares regression models, the residuals are uncorrelated with the fitted values. 

The residual plots in Figure~\ref{fig:residualsARTIST} are overlain with Loess smoothers -- and corresponding 95\% confidence envelopes -- to check for violations of linearity.  There is no issue with the assumption of linearity in the prediction of the confidence interval score, but there is a slight significant dip in the pattern for hypothesis interval residuals.  The assumption of homoscedasticity appear to hold with the residuals spread fairly evenly at all levels of the fitted values. There are however a few outliers on the residual plots, specifically a few students who scored abnormally lower than predicted. These outliers were investigated and found to be low leverage and non-influential. 

<<residualsARTIST, echo=FALSE , message=FALSE, warning=FALSE, fig.width=8, fig.height=3.2, out.width='.9\\linewidth', fig.pos='h',fig.align='center',fig.cap="ARTIST Model residual plots overlaid with Loess smoother and corresponding 95\\% confidence envelopes.">>=
## check residual plots for homoscedesticity and linearity
jitheight <- .1
p1 <- qplot(mod2small$fitted.values[,1] ,mod2small$residuals[,1], geom = "jitter") + 
  geom_hline(yintercept=0) +geom_jitter(position = position_jitter(height = jitheight )) +
  xlab("ARTIST CI Score Fitted Value") + ylab("ARTIST CI Score Residual") +  theme_bw() + geom_smooth()
p2 <- qplot(mod2small$fitted.values[,2] ,mod2small$residuals[,2]) + 
  geom_hline(yintercept=0) +geom_jitter(position = position_jitter(height = jitheight )) +
  xlab("ARTIST HT Score Fitted Value") + ylab("ARTIST HT Score Residual") +  theme_bw() + geom_smooth()
grid.arrange(p1,p2,nrow=1)
@

\subsection{Applied Model Diagnostics}
\label{appAppliedModDiag}

A correlation of $\Sexpr{round(with(dat, cor(AppliedCI,AppliedHT)),3)}$ between the pair of applied problem scores for confidence intervals and hypothesis tests is acceptable for modeling with MANCOVA.  The assumption of normality of errors in the Applied Model is potentially problematic. The normal quantile plot for the residuals from the applied confidence interval score in Figure~\ref{fig:AppliedModDiag} show a distinct curve. The scatterplot of the residual pairs from the Applied Model also appear to have a non-normal bivariate distribution.

The Loess smoothers do not significantly departing from a the horizontal lines at zero for the residual plots in Figure~\ref{fig:residuals2} and thus no departures from the assumption of linearity.  The residuals do however show signs of changing variance over the range of the fitted values. This appears to be driven by the upper bound on student scores for each question.

<<AppliedModDiag, echo=FALSE , warning=FALSE, fig.width=5, fig.height=5, out.width='.49\\linewidth', fig.pos='h',fig.align='center',fig.cap="Normal quantile plots (left) and bivariate scatterplot (right) for residuals of each response from Applied Model.",fig.scap="Normal quantile plots and bivariate scatterplot for residuals of each response from Applied Model.",fig.show='hold'>>=
#--------------------------------------------------
### Check Conditions for MANCOVA linear model fit
## check response correlations to avoid colinearity in multivariate model
#Rule of Thumb: ~ .3 to .55 then MANCOVA will work well
#with(dat, cor(AppliedCI,AppliedHT)) #Not overly correlated

### check for influencial and high leverage points
#infmeas <- influence.measures(mod1small)
#head(infmeas$infmat)
#head(infmeas$is.inf)
### no high leverage

qqp1 <- qplot(sample=mod1small$residuals[,1], stat="qq", xlab="Theoretical Normal Quantiles",
       ylab="Applied CI Score Residual") +  theme_bw()

qqp2 <- qplot(sample=mod1small$residuals[,2], stat="qq", xlab="Theoretical Normal Quantiles",
      ylab="Applied HT Score Residual") +  theme_bw()

grid.arrange(qqp1,qqp2,nrow=2)

qplot(mod1small$residuals[,1],mod1small$residuals[,2],
      xlab="Applied CI Score Residual",  ylab="Applied HT Score Residual")  +  theme_bw()
@


<<residuals2, echo=FALSE , warning=FALSE, message=FALSE, fig.width=8, fig.height=3.2, out.width='.9\\linewidth', fig.pos='H',fig.align='center',fig.cap="Applied Model residual plots overlaid with Loess smoother and corresponding 95\\% confidence envelopes.">>=
p1 <- qplot(mod1small$fitted.values[,1] ,mod1small$residuals[,1]) + 
  geom_hline(yintercept=0) +geom_jitter(position = position_jitter(height = jitheight )) +
  xlab("Applied CI Score Fitted Value") + ylab("Applied CI Score Residual") +  theme_bw() + geom_smooth()
p2 <- qplot(mod1small$fitted.values[,2] ,mod1small$residuals[,2]) + 
  geom_hline(yintercept=0) +geom_jitter(position = position_jitter(height = jitheight )) +
  xlab("Applied HT Score Fitted Value") + ylab("Applied HT Score Residual") +  theme_bw() + geom_smooth()
grid.arrange(p1,p2,nrow=1)
@


\newpage

\bibliography{references}


\end{document}

